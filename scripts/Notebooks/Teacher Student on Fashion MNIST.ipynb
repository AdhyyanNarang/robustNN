{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0610 18:32:41.013946 4613092800 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/cleverhans/utils_tf.py:341: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "import logging\n",
    "import os, random, time, sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.layers as layers\n",
    "import ipdb\n",
    "import keras\n",
    "\n",
    "sys.path.append('../../')\n",
    "import feedforward_robust as ffr\n",
    "\n",
    "sys.path.append('../../utils/')\n",
    "from utils.mnist_corruption import *\n",
    "from utils.utils_models import *\n",
    "from utils.utils_analysis import *\n",
    "from utils.utils_feedforward import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "#Read the counter\n",
    "ctr_file = \"../counter.txt\"\n",
    "f = open(ctr_file, 'r')\n",
    "counter = f.readline()\n",
    "f.close()\n",
    "\n",
    "counter = 1 + int(counter)\n",
    "f = open(ctr_file,'w')\n",
    "f.write('{}'.format(counter))\n",
    "f.close()\n",
    "logfile = \"../logs/results_\" + str(counter) + \".log\"\n",
    "\n",
    "logger = logging.getLogger(\"robustness\")\n",
    "logger.setLevel(logging.DEBUG)\n",
    "fh = logging.FileHandler(logfile)\n",
    "fh.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "fh.setFormatter(formatter)\n",
    "logger.addHandler(fh)\n",
    "logger.addHandler(logging.StreamHandler())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Fashion MNIST Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "x_train = x_train/255\n",
    "x_test = x_test/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One hot the labels\n",
    "num_classes = 10\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)                                                                                         \n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_master, y_train_master = x_train[:30000], y_train[:30000]\n",
    "x_train_slave, y_train_slave = x_train[30000:], y_train[30000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Flatten everything\n",
    "x_train_master_flat, input_shape = flatten_mnist(x_train_master) \n",
    "x_train_slave_flat, _ = flatten_mnist(x_train_slave)\n",
    "x_test_flat, _  = flatten_mnist(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configurations\n",
    "eps_train = 0.1                                                                                                                            \n",
    "eps_test = 0.1                                                                                                                             \n",
    "tensorboard_dir = \"../tb/\"                                                                                                                \n",
    "weights_dir = \"../weights/\"                                                                                                               \n",
    "load_weights = False                                                                                                              \n",
    "load_counter = 234                                                                                                            \n",
    "sigma = tf.nn.relu                                                                                                                         \n",
    "#epochs, reg, lr = 300, 0.00, 1e-4    \n",
    "epochs, reg, lr = 10, 0.00, 1e-3                                                                                                          \n",
    "pgd_eta, pgd_num_iter = 1e-2, 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initialized instance variables of the robust model class\n",
      "I0610 18:32:41.989053 4613092800 feedforward_robust.py:32] Initialized instance variables of the robust model class\n",
      "W0610 18:32:41.990164 4613092800 deprecation_wrapper.py:119] From ../../feedforward_robust.py:36: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "Created placeholders for x and y\n",
      "I0610 18:32:41.992779 4613092800 feedforward_robust.py:38] Created placeholders for x and y\n",
      "W0610 18:32:41.994800 4613092800 deprecation_wrapper.py:119] From ../../utils/utils_feedforward.py:34: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "W0610 18:32:41.995407 4613092800 deprecation_wrapper.py:119] From ../../utils/utils_feedforward.py:34: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
      "\n",
      "W0610 18:32:41.996784 4613092800 deprecation_wrapper.py:119] From ../../utils/utils_feedforward.py:36: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "W0610 18:32:42.018879 4613092800 deprecation_wrapper.py:119] From ../../utils/utils_feedforward.py:40: The name tf.summary.histogram is deprecated. Please use tf.compat.v1.summary.histogram instead.\n",
      "\n",
      "Created layers and tensor for logits\n",
      "I0610 18:32:42.088810 4613092800 feedforward_robust.py:42] Created layers and tensor for logits\n",
      "Added accuracy computation to the graph\n",
      "I0610 18:32:42.095577 4613092800 feedforward_robust.py:46] Added accuracy computation to the graph\n",
      "W0610 18:32:42.096641 4613092800 deprecation_wrapper.py:119] From ../../feedforward_robust.py:47: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
      "\n",
      "W0610 18:32:42.099365 4613092800 deprecation.py:323] From ../../feedforward_robust.py:51: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "Added cross-entropy loss computation to the graph\n",
      "I0610 18:32:42.130125 4613092800 feedforward_robust.py:54] Added cross-entropy loss computation to the graph\n",
      "Model graph was created\n",
      "I0610 18:32:42.135158 4613092800 feedforward_robust.py:60] Model graph was created\n",
      "W0610 18:32:42.136884 4613092800 deprecation_wrapper.py:119] From ../../feedforward_robust.py:61: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      "W0610 18:32:42.193765 4613092800 deprecation_wrapper.py:119] From ../../utils/utils_feedforward.py:78: The name tf.linalg.transpose is deprecated. Please use tf.linalg.matrix_transpose instead.\n",
      "\n",
      "W0610 18:32:42.240143 4613092800 deprecation.py:323] From ../../feedforward_robust.py:737: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Please use tf.global_variables instead.\n",
      "W0610 18:32:42.240978 4613092800 deprecation_wrapper.py:119] From ../../feedforward_robust.py:738: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created model successfully. Now going to train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0610 18:32:42.681906 4613092800 deprecation.py:323] From /usr/local/lib/python3.7/site-packages/tensorflow/python/util/tf_should_use.py:193: initialize_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.variables_initializer` instead.\n",
      "Epoch: 0001    cost: 0.596375283 \n",
      "I0610 18:32:44.054145 4613092800 feedforward_robust.py:714] Epoch: 0001    cost: 0.596375283 \n",
      "Accuracy on batch: 0.812500\n",
      "I0610 18:32:44.055294 4613092800 feedforward_robust.py:715] Accuracy on batch: 0.812500\n",
      "Epoch: 0002    cost: 0.418322287 \n",
      "I0610 18:32:45.059883 4613092800 feedforward_robust.py:714] Epoch: 0002    cost: 0.418322287 \n",
      "Accuracy on batch: 0.843750\n",
      "I0610 18:32:45.061259 4613092800 feedforward_robust.py:715] Accuracy on batch: 0.843750\n",
      "Epoch: 0003    cost: 0.375494492 \n",
      "I0610 18:32:46.022912 4613092800 feedforward_robust.py:714] Epoch: 0003    cost: 0.375494492 \n",
      "Accuracy on batch: 0.812500\n",
      "I0610 18:32:46.024443 4613092800 feedforward_robust.py:715] Accuracy on batch: 0.812500\n",
      "Epoch: 0004    cost: 0.347525236 \n",
      "I0610 18:32:46.920741 4613092800 feedforward_robust.py:714] Epoch: 0004    cost: 0.347525236 \n",
      "Accuracy on batch: 0.812500\n",
      "I0610 18:32:46.922095 4613092800 feedforward_robust.py:715] Accuracy on batch: 0.812500\n",
      "Epoch: 0005    cost: 0.326270651 \n",
      "I0610 18:32:47.902966 4613092800 feedforward_robust.py:714] Epoch: 0005    cost: 0.326270651 \n",
      "Accuracy on batch: 0.875000\n",
      "I0610 18:32:47.904680 4613092800 feedforward_robust.py:715] Accuracy on batch: 0.875000\n",
      "Epoch: 0006    cost: 0.307318133 \n",
      "I0610 18:32:48.898341 4613092800 feedforward_robust.py:714] Epoch: 0006    cost: 0.307318133 \n",
      "Accuracy on batch: 0.812500\n",
      "I0610 18:32:48.899542 4613092800 feedforward_robust.py:715] Accuracy on batch: 0.812500\n",
      "Epoch: 0007    cost: 0.294294165 \n",
      "I0610 18:32:49.768646 4613092800 feedforward_robust.py:714] Epoch: 0007    cost: 0.294294165 \n",
      "Accuracy on batch: 0.812500\n",
      "I0610 18:32:49.769905 4613092800 feedforward_robust.py:715] Accuracy on batch: 0.812500\n",
      "Epoch: 0008    cost: 0.281598710 \n",
      "I0610 18:32:50.649916 4613092800 feedforward_robust.py:714] Epoch: 0008    cost: 0.281598710 \n",
      "Accuracy on batch: 0.843750\n",
      "I0610 18:32:50.651138 4613092800 feedforward_robust.py:715] Accuracy on batch: 0.843750\n",
      "Epoch: 0009    cost: 0.269287487 \n",
      "I0610 18:32:51.514385 4613092800 feedforward_robust.py:714] Epoch: 0009    cost: 0.269287487 \n",
      "Accuracy on batch: 0.875000\n",
      "I0610 18:32:51.515636 4613092800 feedforward_robust.py:715] Accuracy on batch: 0.875000\n",
      "Epoch: 0010    cost: 0.258404453 \n",
      "I0610 18:32:52.345285 4613092800 feedforward_robust.py:714] Epoch: 0010    cost: 0.258404453 \n",
      "Accuracy on batch: 0.937500\n",
      "I0610 18:32:52.346688 4613092800 feedforward_robust.py:715] Accuracy on batch: 0.937500\n",
      "Optimization Finished!\n",
      "I0610 18:32:52.347962 4613092800 feedforward_robust.py:716] Optimization Finished!\n",
      "Final Train Loss 0.255732\n",
      "I0610 18:32:52.479589 4613092800 feedforward_robust.py:724] Final Train Loss 0.255732\n",
      "Final Train Accuracy 0.903433:\n",
      "I0610 18:32:52.480531 4613092800 feedforward_robust.py:725] Final Train Accuracy 0.903433:\n",
      "Model was trained on benign data\n",
      "I0610 18:32:52.482367 4613092800 feedforward_robust.py:743] Model was trained on benign data\n",
      "Model was evaluated on benign data\n",
      "I0610 18:32:52.550459 4613092800 feedforward_robust.py:640] Model was evaluated on benign data\n",
      "Model is being evaluated on FGSM data\n",
      "I0610 18:32:52.690109 4613092800 feedforward_robust.py:647] Model is being evaluated on FGSM data\n",
      "Model is being evaluated on PGD points generated using 0.010000 learning rate and 50 iterations\n",
      "I0610 18:32:52.710998 4613092800 feedforward_robust.py:649] Model is being evaluated on PGD points generated using 0.010000 learning rate and 50 iterations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Regular test accuracy and loss ----\n",
      "(0.39057168, 0.8655)\n",
      "----FGSM test accuracy and loss ----\n",
      "(9.383926, 0.0323)\n",
      "iteration: 0\n",
      "loss 0.873708\n",
      "iteration: 1\n",
      "loss 1.650008\n",
      "iteration: 2\n",
      "loss 2.705885\n",
      "iteration: 3\n",
      "loss 3.958185\n",
      "iteration: 4\n",
      "loss 5.308160\n",
      "iteration: 5\n",
      "loss 6.700696\n",
      "iteration: 6\n",
      "loss 8.099629\n",
      "iteration: 7\n",
      "loss 9.472996\n",
      "iteration: 8\n",
      "loss 10.794743\n",
      "iteration: 9\n",
      "loss 12.053759\n",
      "iteration: 10\n",
      "loss 13.247781\n",
      "iteration: 11\n",
      "loss 14.371333\n",
      "iteration: 12\n",
      "loss 15.419714\n",
      "iteration: 13\n",
      "loss 16.387943\n",
      "iteration: 14\n",
      "loss 17.276617\n",
      "iteration: 15\n",
      "loss 18.082125\n",
      "iteration: 16\n",
      "loss 18.800011\n",
      "iteration: 17\n",
      "loss 19.423288\n",
      "iteration: 18\n",
      "loss 19.951204\n",
      "iteration: 19\n",
      "loss 20.376629\n",
      "iteration: 20\n",
      "loss 20.734335\n",
      "iteration: 21\n",
      "loss 21.048632\n",
      "iteration: 22\n",
      "loss 21.324717\n",
      "iteration: 23\n",
      "loss 21.568668\n",
      "iteration: 24\n",
      "loss 21.781284\n",
      "iteration: 25\n",
      "loss 21.966791\n",
      "iteration: 26\n",
      "loss 22.125935\n",
      "iteration: 27\n",
      "loss 22.265978\n",
      "iteration: 28\n",
      "loss 22.387779\n",
      "iteration: 29\n",
      "loss 22.495520\n",
      "iteration: 30\n",
      "loss 22.589088\n",
      "iteration: 31\n",
      "loss 22.673775\n",
      "iteration: 32\n",
      "loss 22.748547\n",
      "iteration: 33\n",
      "loss 22.815201\n",
      "iteration: 34\n",
      "loss 22.874847\n",
      "iteration: 35\n",
      "loss 22.928801\n",
      "iteration: 36\n",
      "loss 22.976881\n",
      "iteration: 37\n",
      "loss 23.021212\n",
      "iteration: 38\n",
      "loss 23.061659\n",
      "iteration: 39\n",
      "loss 23.097769\n",
      "iteration: 40\n",
      "loss 23.132057\n",
      "iteration: 41\n",
      "loss 23.162912\n",
      "iteration: 42\n",
      "loss 23.192219\n",
      "iteration: 43\n",
      "loss 23.218382\n",
      "iteration: 44\n",
      "loss 23.242151\n",
      "iteration: 45\n",
      "loss 23.266216\n",
      "iteration: 46\n",
      "loss 23.285513\n",
      "iteration: 47\n",
      "loss 23.307365\n",
      "iteration: 48\n",
      "loss 23.324194\n",
      "iteration: 49\n",
      "loss 23.342751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This is to confirm that attack does not violate constraints\n",
      "I0610 18:32:58.179039 4613092800 feedforward_robust.py:494] This is to confirm that attack does not violate constraints\n",
      "Should be no more than eps\n",
      "I0610 18:32:58.180274 4613092800 feedforward_robust.py:495] Should be no more than eps\n",
      "0.10000005352730845\n",
      "I0610 18:32:58.225305 4613092800 feedforward_robust.py:496] 0.10000005352730845\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----PGD test accuracy and loss ----\n",
      "(23.34275, 0.0015)\n"
     ]
    }
   ],
   "source": [
    "#Setup - Dataset stuff\n",
    "sess = tf.Session()\n",
    "hidden_sizes = [64, 64, 32]\n",
    "dataset = ((x_train_master_flat, y_train_master), (x_test_flat, y_test))\n",
    "\n",
    "scope_name = \"teacher_student_fashion\"\n",
    "if not load_weights:\n",
    "    with tf.variable_scope(scope_name, reuse = False) as scope:\n",
    "\n",
    "        logdir = tensorboard_dir + str(counter)\n",
    "\n",
    "        #Create model\n",
    "        writer = tf.summary.FileWriter(logdir)\n",
    "        model = ffr.RobustMLP(input_shape, hidden_sizes, num_classes, writer = writer, scope = scope_name, logger = logger, sigma = sigma)\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        print(\"Created model successfully. Now going to train\")\n",
    "    \n",
    "        #Train model\n",
    "        model.fit(sess, x_train_master_flat, y_train_master, training_epochs = epochs, reg_op = reg , lr = lr)\n",
    "        \n",
    "        \"\"\"\n",
    "        #Save weights\n",
    "        weights = tf.trainable_variables()\n",
    "        #weights = model.get_weights()[0] + model.get_weights()[1]\n",
    "        saver = tf.train.Saver(weights)\n",
    "        weights_path = saver.save(sess, weights_dir + \"model_\" + str(counter) + \".ckpt\")\n",
    "        print(\"Saved model at %s\"%weights_path)\n",
    "        \"\"\"\n",
    "        \n",
    "        #Test model - regular, fgsm adv, pgd adv\n",
    "        loss_reg, acc_reg = model.evaluate(sess, x_test_flat, y_test)\n",
    "        print(\"----Regular test accuracy and loss ----\")\n",
    "        print((loss_reg, acc_reg))\n",
    "        \n",
    "        loss_fgsm, acc_fgsm = model.adv_evaluate(sess, x_test_flat, y_test, eps_test, pgd = False)\n",
    "        print(\"----FGSM test accuracy and loss ----\")\n",
    "        print((loss_fgsm, acc_fgsm))\n",
    "        \n",
    "        loss_pgd, acc_pgd = model.adv_evaluate(sess, x_test_flat, y_test, eps_test, pgd = True, eta=pgd_eta, num_iter = pgd_num_iter)\n",
    "        print(\"----PGD test accuracy and loss ----\")\n",
    "        print((loss_pgd , acc_pgd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate data for slave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model was evaluated on benign data\n",
      "I0610 18:32:58.327879 4613092800 feedforward_robust.py:640] Model was evaluated on benign data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Regular test accuracy and loss ----\n",
      "(0.39057168, 0.8655)\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope(scope_name, reuse = False) as scope:\n",
    "        loss_reg, acc_reg = model.evaluate(sess, x_test_flat, y_test)\n",
    "        print(\"----Regular test accuracy and loss ----\")\n",
    "        print((loss_reg, acc_reg))\n",
    "        \n",
    "        z_train_slave = model.get_prediction(sess, x_train_slave_flat)\n",
    "        z_test_slave = model.get_prediction(sess, x_test_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.21138227,   1.0028282 ,  -3.4137535 ,   8.74303   ,\n",
       "        -0.61716294, -20.783663  ,  -2.7557034 , -21.851446  ,\n",
       "        -6.4539423 , -11.170397  ], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_train_slave[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train slave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initialized instance variables of the robust model class\n",
      "I0610 18:36:27.793619 4613092800 feedforward_robust.py:32] Initialized instance variables of the robust model class\n",
      "Created placeholders for x and y\n",
      "I0610 18:36:27.798917 4613092800 feedforward_robust.py:38] Created placeholders for x and y\n",
      "Created layers and tensor for logits\n",
      "I0610 18:36:27.881392 4613092800 feedforward_robust.py:42] Created layers and tensor for logits\n",
      "Added accuracy computation to the graph\n",
      "I0610 18:36:27.888337 4613092800 feedforward_robust.py:46] Added accuracy computation to the graph\n",
      "Added MSE loss computation to the graph\n",
      "I0610 18:36:27.894678 4613092800 feedforward_robust.py:58] Added MSE loss computation to the graph\n",
      "Model graph was created\n",
      "I0610 18:36:27.895742 4613092800 feedforward_robust.py:60] Model graph was created\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created model successfully. Now going to train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 0001    cost: 1354.812823447 \n",
      "I0610 18:36:29.512566 4613092800 feedforward_robust.py:714] Epoch: 0001    cost: 1354.812823447 \n",
      "Accuracy on batch: 0.875000\n",
      "I0610 18:36:29.514161 4613092800 feedforward_robust.py:715] Accuracy on batch: 0.875000\n",
      "Epoch: 0002    cost: 330.429121703 \n",
      "I0610 18:36:30.409995 4613092800 feedforward_robust.py:714] Epoch: 0002    cost: 330.429121703 \n",
      "Accuracy on batch: 0.906250\n",
      "I0610 18:36:30.411363 4613092800 feedforward_robust.py:715] Accuracy on batch: 0.906250\n",
      "Epoch: 0003    cost: 239.747939445 \n",
      "I0610 18:36:31.324705 4613092800 feedforward_robust.py:714] Epoch: 0003    cost: 239.747939445 \n",
      "Accuracy on batch: 0.906250\n",
      "I0610 18:36:31.325858 4613092800 feedforward_robust.py:715] Accuracy on batch: 0.906250\n",
      "Epoch: 0004    cost: 191.715804565 \n",
      "I0610 18:36:32.218352 4613092800 feedforward_robust.py:714] Epoch: 0004    cost: 191.715804565 \n",
      "Accuracy on batch: 0.937500\n",
      "I0610 18:36:32.219813 4613092800 feedforward_robust.py:715] Accuracy on batch: 0.937500\n",
      "Epoch: 0005    cost: 162.198778102 \n",
      "I0610 18:36:33.163465 4613092800 feedforward_robust.py:714] Epoch: 0005    cost: 162.198778102 \n",
      "Accuracy on batch: 0.968750\n",
      "I0610 18:36:33.164767 4613092800 feedforward_robust.py:715] Accuracy on batch: 0.968750\n",
      "Epoch: 0006    cost: 142.396145506 \n",
      "I0610 18:36:34.085394 4613092800 feedforward_robust.py:714] Epoch: 0006    cost: 142.396145506 \n",
      "Accuracy on batch: 0.968750\n",
      "I0610 18:36:34.086673 4613092800 feedforward_robust.py:715] Accuracy on batch: 0.968750\n",
      "Epoch: 0007    cost: 127.999266454 \n",
      "I0610 18:36:35.024293 4613092800 feedforward_robust.py:714] Epoch: 0007    cost: 127.999266454 \n",
      "Accuracy on batch: 0.968750\n",
      "I0610 18:36:35.027526 4613092800 feedforward_robust.py:715] Accuracy on batch: 0.968750\n",
      "Epoch: 0008    cost: 117.035903809 \n",
      "I0610 18:36:35.993789 4613092800 feedforward_robust.py:714] Epoch: 0008    cost: 117.035903809 \n",
      "Accuracy on batch: 0.968750\n",
      "I0610 18:36:35.994982 4613092800 feedforward_robust.py:715] Accuracy on batch: 0.968750\n",
      "Epoch: 0009    cost: 108.761531191 \n",
      "I0610 18:36:36.920462 4613092800 feedforward_robust.py:714] Epoch: 0009    cost: 108.761531191 \n",
      "Accuracy on batch: 0.968750\n",
      "I0610 18:36:36.922233 4613092800 feedforward_robust.py:715] Accuracy on batch: 0.968750\n",
      "Epoch: 0010    cost: 102.333988100 \n",
      "I0610 18:36:37.827517 4613092800 feedforward_robust.py:714] Epoch: 0010    cost: 102.333988100 \n",
      "Accuracy on batch: 0.968750\n",
      "I0610 18:36:37.828837 4613092800 feedforward_robust.py:715] Accuracy on batch: 0.968750\n",
      "Optimization Finished!\n",
      "I0610 18:36:37.831621 4613092800 feedforward_robust.py:716] Optimization Finished!\n",
      "Final Train Loss 93800.468750\n",
      "I0610 18:36:37.976907 4613092800 feedforward_robust.py:724] Final Train Loss 93800.468750\n",
      "Final Train Accuracy 0.934033:\n",
      "I0610 18:36:37.978454 4613092800 feedforward_robust.py:725] Final Train Accuracy 0.934033:\n",
      "Model was trained on benign data\n",
      "I0610 18:36:37.983753 4613092800 feedforward_robust.py:743] Model was trained on benign data\n",
      "Model was evaluated on benign data\n",
      "I0610 18:36:38.029007 4613092800 feedforward_robust.py:640] Model was evaluated on benign data\n",
      "Model is being evaluated on FGSM data\n",
      "I0610 18:36:38.186693 4613092800 feedforward_robust.py:647] Model is being evaluated on FGSM data\n",
      "Model is being evaluated on PGD points generated using 0.010000 learning rate and 50 iterations\n",
      "I0610 18:36:38.203843 4613092800 feedforward_robust.py:649] Model is being evaluated on PGD points generated using 0.010000 learning rate and 50 iterations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Regular test accuracy and loss ----\n",
      "(3963251.5, 0.8538)\n",
      "----FGSM test accuracy and loss ----\n",
      "(20922640.0, 0.8099)\n",
      "iteration: 0\n",
      "loss 0.863301\n",
      "iteration: 1\n",
      "loss 1.551709\n",
      "iteration: 2\n",
      "loss 2.482691\n",
      "iteration: 3\n",
      "loss 3.588220\n",
      "iteration: 4\n",
      "loss 4.774253\n",
      "iteration: 5\n",
      "loss 5.984824\n",
      "iteration: 6\n",
      "loss 7.195096\n",
      "iteration: 7\n",
      "loss 8.386842\n",
      "iteration: 8\n",
      "loss 9.547078\n",
      "iteration: 9\n",
      "loss 10.660930\n",
      "iteration: 10\n",
      "loss 11.718728\n",
      "iteration: 11\n",
      "loss 12.710370\n",
      "iteration: 12\n",
      "loss 13.630534\n",
      "iteration: 13\n",
      "loss 14.476028\n",
      "iteration: 14\n",
      "loss 15.243172\n",
      "iteration: 15\n",
      "loss 15.929166\n",
      "iteration: 16\n",
      "loss 16.530148\n",
      "iteration: 17\n",
      "loss 17.042925\n",
      "iteration: 18\n",
      "loss 17.465288\n",
      "iteration: 19\n",
      "loss 17.793537\n",
      "iteration: 20\n",
      "loss 18.060768\n",
      "iteration: 21\n",
      "loss 18.293888\n",
      "iteration: 22\n",
      "loss 18.499531\n",
      "iteration: 23\n",
      "loss 18.679527\n",
      "iteration: 24\n",
      "loss 18.833565\n",
      "iteration: 25\n",
      "loss 18.966845\n",
      "iteration: 26\n",
      "loss 19.081621\n",
      "iteration: 27\n",
      "loss 19.180250\n",
      "iteration: 28\n",
      "loss 19.265211\n",
      "iteration: 29\n",
      "loss 19.340038\n",
      "iteration: 30\n",
      "loss 19.403435\n",
      "iteration: 31\n",
      "loss 19.460836\n",
      "iteration: 32\n",
      "loss 19.512028\n",
      "iteration: 33\n",
      "loss 19.557270\n",
      "iteration: 34\n",
      "loss 19.597002\n",
      "iteration: 35\n",
      "loss 19.632948\n",
      "iteration: 36\n",
      "loss 19.665796\n",
      "iteration: 37\n",
      "loss 19.695541\n",
      "iteration: 38\n",
      "loss 19.721912\n",
      "iteration: 39\n",
      "loss 19.747593\n",
      "iteration: 40\n",
      "loss 19.770123\n",
      "iteration: 41\n",
      "loss 19.791410\n",
      "iteration: 42\n",
      "loss 19.810898\n",
      "iteration: 43\n",
      "loss 19.828878\n",
      "iteration: 44\n",
      "loss 19.845045\n",
      "iteration: 45\n",
      "loss 19.860865\n",
      "iteration: 46\n",
      "loss 19.874769\n",
      "iteration: 47\n",
      "loss 19.887371\n",
      "iteration: 48\n",
      "loss 19.899961\n",
      "iteration: 49\n",
      "loss 19.910448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This is to confirm that attack does not violate constraints\n",
      "I0610 18:36:43.594653 4613092800 feedforward_robust.py:494] This is to confirm that attack does not violate constraints\n",
      "Should be no more than eps\n",
      "I0610 18:36:43.599056 4613092800 feedforward_robust.py:495] Should be no more than eps\n",
      "0.10000005352730845\n",
      "I0610 18:36:43.645316 4613092800 feedforward_robust.py:496] 0.10000005352730845\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----PGD test accuracy and loss ----\n",
      "(9183898.0, 0.0041)\n"
     ]
    }
   ],
   "source": [
    "#Setup - Dataset stuff\n",
    "def slave_training():\n",
    "    tf.reset_default_graph()\n",
    "    sess = tf.Session()\n",
    "    hidden_sizes = [64, 64, 32]\n",
    "    dataset = ((x_train_slave_flat, z_train_slave), (x_test_flat, y_test))\n",
    "\n",
    "    scope_name = \"teacher_student_fashion\"\n",
    "    if not load_weights:\n",
    "        with tf.variable_scope(scope_name, reuse = tf.AUTO_REUSE) as scope:\n",
    "\n",
    "            logdir = tensorboard_dir + str(counter)\n",
    "\n",
    "            #Create model\n",
    "            writer = tf.summary.FileWriter(logdir)\n",
    "            model = ffr.RobustMLP(input_shape, hidden_sizes, num_classes, writer = writer, scope = scope_name, logger = logger, sigma = sigma, classification = False)\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            print(\"Created model successfully. Now going to train\")\n",
    "\n",
    "            #Train model\n",
    "            model.fit(sess, x_train_slave_flat, z_train_slave, training_epochs = epochs, reg_op = reg , lr = lr)\n",
    "\n",
    "            \"\"\"\n",
    "            #Save weights\n",
    "            weights = tf.trainable_variables()\n",
    "            #weights = model.get_weights()[0] + model.get_weights()[1]\n",
    "            saver = tf.train.Saver(weights)\n",
    "            weights_path = saver.save(sess, weights_dir + \"model_\" + str(counter) + \".ckpt\")\n",
    "            print(\"Saved model at %s\"%weights_path)\n",
    "            \"\"\"\n",
    "\n",
    "            #Test model - regular, fgsm adv, pgd adv\n",
    "            loss_real_reg, acc_real_reg = model.evaluate(sess, x_test_flat, z_test_slave)\n",
    "            print(\"----Regular test accuracy and loss ----\")\n",
    "            print((loss_real_reg, acc_real_reg))\n",
    "            \n",
    "            loss_class_reg, acc_reg = model.evaluate(sess, x_test_flat, y_test)\n",
    "            print(\"----Real test accuracy and loss comparing to teacher ----\")\n",
    "            print((loss_class_reg, acc_reg))\n",
    "\n",
    "            loss_fgsm, acc_fgsm = model.adv_evaluate(sess, x_test_flat, y_test, eps_test, pgd = False)\n",
    "            print(\"----FGSM test accuracy and loss ----\")\n",
    "            print((loss_fgsm, acc_fgsm))\n",
    "\n",
    "            loss_pgd, acc_pgd = model.adv_evaluate(sess, x_test_flat, y_test, eps_test, pgd = True, eta=pgd_eta, num_iter = pgd_num_iter)\n",
    "            print(\"----PGD test accuracy and loss ----\")\n",
    "            print((loss_pgd , acc_pgd))\n",
    "            \n",
    "            return loss_reg, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
