{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0704 22:49:58.390467 4642125248 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/cleverhans/utils_tf.py:341: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "import logging\n",
    "import os, random, time, sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.layers as layers\n",
    "import ipdb\n",
    "import keras\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append('../../../')\n",
    "import feedforward_robust as ffr\n",
    "\n",
    "sys.path.append('../../../utils/')\n",
    "from utils.mnist_corruption import *\n",
    "from utils.utils_models import *\n",
    "from utils.utils_analysis import *\n",
    "from utils.utils_feedforward import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "#Read the counter\n",
    "ctr_file = \"../../counter.txt\"\n",
    "f = open(ctr_file, 'r')\n",
    "counter = f.readline()\n",
    "f.close()\n",
    "\n",
    "counter = 1 + int(counter)\n",
    "f = open(ctr_file,'w')\n",
    "f.write('{}'.format(counter))\n",
    "f.close()\n",
    "logfile = \"../../logs/results_\" + str(counter) + \".log\"\n",
    "\n",
    "logger = logging.getLogger(\"robustness\")\n",
    "logger.setLevel(logging.DEBUG)\n",
    "fh = logging.FileHandler(logfile)\n",
    "fh.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "fh.setFormatter(formatter)\n",
    "logger.addHandler(fh)\n",
    "logger.addHandler(logging.StreamHandler())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Fashion MNIST Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "x_train = x_train/255\n",
    "x_test = x_test/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One hot the labels\n",
    "num_classes = 10\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)                                                                                         \n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_master, y_train_master = x_train[:30000], y_train[:30000]\n",
    "x_train_slave, y_train_slave = x_train[30000:], y_train[30000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Flatten everything\n",
    "x_train_master_flat, input_shape = flatten_mnist(x_train_master) \n",
    "x_train_slave_flat, _ = flatten_mnist(x_train_slave)\n",
    "x_test_flat, _  = flatten_mnist(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_shape = x_test_flat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_random_flat = np.random.uniform(0, 1, test_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configurations\n",
    "eps_train = 0.1                                                                                                                            \n",
    "eps_test = 0.1                                                                                                                             \n",
    "tensorboard_dir = \"../tb/\"                                                                                                                \n",
    "weights_dir = \"../weights/\"                                                                                                               \n",
    "load_weights = False                                                                                                              \n",
    "load_counter = 234                                                                                                            \n",
    "sigma = tf.nn.relu                                                                                                                         \n",
    "epochs, reg, lr = 3, 0.00, 1e-3    \n",
    "#epochs, reg, lr = 30, 0.00, 15e-4                                                                                                          \n",
    "pgd_eta, pgd_num_iter = 1e-2, 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initialized instance variables of the robust model class\n",
      "I0704 22:50:00.343858 4642125248 feedforward_robust.py:32] Initialized instance variables of the robust model class\n",
      "W0704 22:50:00.345285 4642125248 deprecation_wrapper.py:119] From ../../../feedforward_robust.py:36: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "Created placeholders for x and y\n",
      "I0704 22:50:00.367156 4642125248 feedforward_robust.py:40] Created placeholders for x and y\n",
      "W0704 22:50:00.368518 4642125248 deprecation_wrapper.py:119] From ../../../utils/utils_feedforward.py:34: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "W0704 22:50:00.387407 4642125248 deprecation_wrapper.py:119] From ../../../utils/utils_feedforward.py:34: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
      "\n",
      "W0704 22:50:00.400591 4642125248 deprecation_wrapper.py:119] From ../../../utils/utils_feedforward.py:36: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "W0704 22:50:00.460621 4642125248 deprecation_wrapper.py:119] From ../../../utils/utils_feedforward.py:40: The name tf.summary.histogram is deprecated. Please use tf.compat.v1.summary.histogram instead.\n",
      "\n",
      "Created layers and tensor for logits\n",
      "I0704 22:50:00.537092 4642125248 feedforward_robust.py:44] Created layers and tensor for logits\n",
      "Added accuracy computation to the graph\n",
      "I0704 22:50:00.550345 4642125248 feedforward_robust.py:48] Added accuracy computation to the graph\n",
      "W0704 22:50:00.551736 4642125248 deprecation_wrapper.py:119] From ../../../feedforward_robust.py:49: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
      "\n",
      "W0704 22:50:00.554961 4642125248 deprecation.py:323] From ../../../feedforward_robust.py:53: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "Added cross-entropy loss computation to the graph\n",
      "I0704 22:50:00.603604 4642125248 feedforward_robust.py:56] Added cross-entropy loss computation to the graph\n",
      "Model graph was created\n",
      "I0704 22:50:00.604613 4642125248 feedforward_robust.py:62] Model graph was created\n",
      "W0704 22:50:00.607465 4642125248 deprecation_wrapper.py:119] From ../../../feedforward_robust.py:63: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      "W0704 22:50:00.716991 4642125248 deprecation_wrapper.py:119] From ../../../utils/utils_feedforward.py:78: The name tf.linalg.transpose is deprecated. Please use tf.linalg.matrix_transpose instead.\n",
      "\n",
      "W0704 22:50:00.756841 4642125248 deprecation.py:323] From ../../../feedforward_robust.py:778: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Please use tf.global_variables instead.\n",
      "W0704 22:50:00.757490 4642125248 deprecation_wrapper.py:119] From ../../../feedforward_robust.py:779: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created model successfully. Now going to train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0704 22:50:01.329941 4642125248 deprecation.py:323] From /usr/local/lib/python3.7/site-packages/tensorflow/python/util/tf_should_use.py:193: initialize_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.variables_initializer` instead.\n",
      "Epoch: 0001    cost: 0.602290839 \n",
      "I0704 22:50:02.773890 4642125248 feedforward_robust.py:751] Epoch: 0001    cost: 0.602290839 \n",
      "Accuracy on batch: 0.812500\n",
      "I0704 22:50:02.775295 4642125248 feedforward_robust.py:752] Accuracy on batch: 0.812500\n",
      "Epoch: 0002    cost: 0.423337503 \n",
      "I0704 22:50:03.745182 4642125248 feedforward_robust.py:751] Epoch: 0002    cost: 0.423337503 \n",
      "Accuracy on batch: 0.843750\n",
      "I0704 22:50:03.746630 4642125248 feedforward_robust.py:752] Accuracy on batch: 0.843750\n",
      "Epoch: 0003    cost: 0.379834467 \n",
      "I0704 22:50:04.694087 4642125248 feedforward_robust.py:751] Epoch: 0003    cost: 0.379834467 \n",
      "Accuracy on batch: 0.875000\n",
      "I0704 22:50:04.695171 4642125248 feedforward_robust.py:752] Accuracy on batch: 0.875000\n",
      "Optimization Finished!\n",
      "I0704 22:50:04.696726 4642125248 feedforward_robust.py:753] Optimization Finished!\n",
      "Final Train Loss 0.360292\n",
      "I0704 22:50:04.846853 4642125248 feedforward_robust.py:761] Final Train Loss 0.360292\n",
      "Final Train Accuracy 0.866267:\n",
      "I0704 22:50:04.847904 4642125248 feedforward_robust.py:762] Final Train Accuracy 0.866267:\n",
      "Model was trained on benign data\n",
      "I0704 22:50:04.848900 4642125248 feedforward_robust.py:784] Model was trained on benign data\n",
      "Model was evaluated on benign data\n",
      "I0704 22:50:05.068540 4642125248 feedforward_robust.py:663] Model was evaluated on benign data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Weight norms ----\n",
      "[17.560528, 9.236234, 7.4632244, 4.8046856]\n",
      "----Regular test accuracy and loss ----\n",
      "(0.4298507, 0.8453)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model is being evaluated on FGSM data\n",
      "I0704 22:50:05.205718 4642125248 feedforward_robust.py:670] Model is being evaluated on FGSM data\n",
      "Model is being evaluated on PGD points generated using 0.010000 learning rate and 50 iterations\n",
      "I0704 22:50:05.278076 4642125248 feedforward_robust.py:672] Model is being evaluated on PGD points generated using 0.010000 learning rate and 50 iterations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----FGSM test accuracy and loss ----\n",
      "(7.5254445, 0.0365)\n",
      "iteration: 0\n",
      "loss 0.748103\n",
      "iteration: 20\n",
      "loss 11.352206\n",
      "iteration: 40\n",
      "loss 12.163715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This is to confirm that attack does not violate constraints\n",
      "I0704 22:50:10.112802 4642125248 feedforward_robust.py:517] This is to confirm that attack does not violate constraints\n",
      "Should be no more than eps\n",
      "I0704 22:50:10.113985 4642125248 feedforward_robust.py:518] Should be no more than eps\n",
      "0.10000005352730845\n",
      "I0704 22:50:10.214136 4642125248 feedforward_robust.py:519] 0.10000005352730845\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----PGD test accuracy and loss ----\n",
      "(12.236209, 0.0109)\n"
     ]
    }
   ],
   "source": [
    "#Setup - Dataset stuff\n",
    "tf.reset_default_graph()\n",
    "sess = tf.Session()\n",
    "hidden_sizes = [64, 64, 32]\n",
    "dataset = ((x_train_master_flat, y_train_master), (x_test_flat, y_test))\n",
    "\n",
    "scope_name = \"teacher_student_fashion\"\n",
    "if not load_weights:\n",
    "    with tf.variable_scope(scope_name, reuse = False) as scope:\n",
    "\n",
    "        logdir = tensorboard_dir + str(counter)\n",
    "\n",
    "        #Create model\n",
    "        writer = tf.summary.FileWriter(logdir)\n",
    "        model = ffr.RobustMLP(input_shape, hidden_sizes, num_classes, writer = writer, scope = scope_name, logger = logger, sigma = sigma)\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        print(\"Created model successfully. Now going to train\")\n",
    "    \n",
    "        #Train model\n",
    "        model.fit(sess, x_train_master_flat, y_train_master, training_epochs = epochs, reg_op = reg , lr = lr)\n",
    "        \n",
    "        margins_master = model.get_pointwise_margin(sess, x_train_master_flat, y_train)\n",
    "        \n",
    "        weight_norms = model.get_weight_norms(sess)\n",
    "        print(\"----- Weight norms ----\")\n",
    "        print(weight_norms)\n",
    "        \n",
    "        \"\"\"\n",
    "        #Save weights\n",
    "        weights = tf.trainable_variables()\n",
    "        #weights = model.get_weights()[0] + model.get_weights()[1]\n",
    "        saver = tf.train.Saver(weights)\n",
    "        weights_path = saver.save(sess, weights_dir + \"model_\" + str(counter) + \".ckpt\")\n",
    "        print(\"Saved model at %s\"%weights_path)\n",
    "        \"\"\"\n",
    "        \n",
    "        #Test model - regular, fgsm adv, pgd adv\n",
    "        loss_reg, acc_reg = model.evaluate(sess, x_test_flat, y_test)\n",
    "        print(\"----Regular test accuracy and loss ----\")\n",
    "        print((loss_reg, acc_reg))\n",
    "        \n",
    "        loss_fgsm, acc_fgsm, deltas_fgsm = model.adv_evaluate(sess, x_test_flat, y_test, eps_test, pgd = False)\n",
    "        print(\"----FGSM test accuracy and loss ----\")\n",
    "        print((loss_fgsm, acc_fgsm))\n",
    "        \n",
    "        loss_pgd, acc_pgd, deltas_pgd = model.adv_evaluate(sess, x_test_flat, y_test, eps_test, pgd = True, eta=pgd_eta, num_iter = pgd_num_iter)\n",
    "        print(\"----PGD test accuracy and loss ----\")\n",
    "        print((loss_pgd , acc_pgd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observe deltas pgd. Are they at boundary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 784)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deltas_pgd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7840000,)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "deltas_flat = deltas_pgd.flatten()\n",
    "print(deltas_flat.shape)\n",
    "plt.hist(deltas_flat, 20)\n",
    "plt.ticklabel_format(axis=\"y\", style=\"sci\", scilimits=(0,0))\n",
    "plt.xlabel(\"Delta values\")\n",
    "plt.ylabel(\"Number of pixels\")\n",
    "plt.savefig(\"/Users/adhyyan/Desktop/delta_hist.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect margins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "margins_master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(margins_master, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate data for slave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(scope_name, reuse = False) as scope:\n",
    "        loss_reg, acc_reg = model.evaluate(sess, x_test_flat, y_test)\n",
    "        print(\"----Regular test accuracy and loss ----\")\n",
    "        print((loss_reg, acc_reg))\n",
    "        \n",
    "        z_train_slave = model.get_prediction(sess, x_train_slave_flat)\n",
    "        z_test_slave = model.get_prediction(sess, x_test_flat)\n",
    "        z_test_random_master = model.get_prediction(sess, x_test_random_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_train_slave[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train slave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup - Dataset stuff\n",
    "def slave_training():\n",
    "    epochs = 30\n",
    "    lr = 15e-4\n",
    "    tf.reset_default_graph()\n",
    "    sess = tf.Session()\n",
    "    hidden_sizes = [64, 64, 32]\n",
    "    dataset = ((x_train_slave_flat, z_train_slave), (x_test_flat, y_test))\n",
    "\n",
    "    scope_name = \"teacher_student_fashion\"\n",
    "    if not load_weights:\n",
    "        with tf.variable_scope(scope_name, reuse = tf.AUTO_REUSE) as scope:\n",
    "\n",
    "            logdir = tensorboard_dir + str(counter)\n",
    "\n",
    "            #Create model\n",
    "            writer = tf.summary.FileWriter(logdir)\n",
    "            model = ffr.RobustMLP(input_shape, hidden_sizes, num_classes, writer = writer, scope = scope_name, logger = logger, sigma = sigma, classification = False)\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            print(\"Created model successfully. Now going to train\")\n",
    "\n",
    "            #Train model\n",
    "            model.fit(sess, x_train_slave_flat, z_train_slave, training_epochs = epochs, reg_op = reg , lr = lr)\n",
    "\n",
    "            \"\"\"\n",
    "            #Save weights\n",
    "            weights = tf.trainable_variables()\n",
    "            #weights = model.get_weights()[0] + model.get_weights()[1]\n",
    "            saver = tf.train.Saver(weights)\n",
    "            weights_path = saver.save(sess, weights_dir + \"model_\" + str(counter) + \".ckpt\")\n",
    "            print(\"Saved model at %s\"%weights_path)\n",
    "            \"\"\"\n",
    "            loss_real_train, acc_train = model.evaluate(sess, x_train_slave_flat, z_train_slave)\n",
    "\n",
    "            #Test model - regular, fgsm adv, pgd adv\n",
    "            loss_real_reg, acc_real_reg = model.evaluate(sess, x_test_flat, z_test_slave)\n",
    "            print(\"----Real test loss and accuracy comparing to teacher ----\")\n",
    "            print((loss_real_reg, acc_real_reg))\n",
    "            \n",
    "            loss_class_reg, acc_reg = model.evaluate(sess, x_test_flat, y_test)\n",
    "            print(\"----Regular test loss and accuracy ----\")\n",
    "            print((loss_class_reg, acc_reg))\n",
    "\n",
    "            loss_fgsm, acc_fgsm = model.adv_evaluate(sess, x_test_flat, y_test, eps_test, pgd = False)\n",
    "            print(\"----FGSM test loss and accuracy ----\")\n",
    "            print((loss_fgsm, acc_fgsm))\n",
    "\n",
    "            loss_pgd, acc_pgd = model.adv_evaluate(sess, x_test_flat, y_test, eps_test, pgd = True, eta=pgd_eta, num_iter = pgd_num_iter)\n",
    "            print(\"----PGD test loss and accuracy ----\")\n",
    "            print((loss_pgd , acc_pgd))\n",
    "            \n",
    "            slave_train_confidences = model.get_prediction(sess, x_train_slave_flat)\n",
    "            slave_test_confidences = model.get_prediction(sess, x_test_flat)\n",
    "            slave_random_confidences = model.get_prediction(sess, x_test_random_flat)\n",
    "            \n",
    "            return loss_real_train, acc_train, loss_real_reg, acc_real_reg, acc_reg, slave_train_confidences, slave_test_confidences, slave_random_confidences\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tup = slave_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(tup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df['mse on z_train'] = []\n",
    "df['acc on z_train'] = []\n",
    "df['mse on z_test'] = []\n",
    "df['acc on z_test'] = []\n",
    "df['acc on y_test'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_confidences = []\n",
    "test_confidences = []\n",
    "random_confidences = []\n",
    "for i in range(3):\n",
    "    loss_real_train, acc_train, loss_real_reg, acc_real_reg, acc_reg, slave_train_confidences, slave_test_confidences, random_conf = slave_training()\n",
    "    df.loc[i] = [loss_real_train, acc_train, loss_real_reg, acc_real_reg, acc_reg]\n",
    "    train_confidences.append(slave_train_confidences)\n",
    "    test_confidences.append(slave_test_confidences)\n",
    "    random_confidences.append(random_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_test = len(x_test)\n",
    "idx_random = np.random.choice(num_test, 10)\n",
    "for idx in idx_random:\n",
    "    x = np.arange(10)\n",
    "    width = 0.2  # the width of the bars\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.bar(x - width/2, scipy.special.softmax(random_confidences[0][idx]), width, label = 'slave 1')\n",
    "    ax.bar(x - 3*width/2, scipy.special.softmax(random_confidences[1][idx]), width, label = 'slave 2')\n",
    "    ax.bar(x + width/2, scipy.special.softmax(random_confidences[2][idx]), width, label = 'slave 3')\n",
    "    ax.bar(x + 3*width/2, scipy.special.softmax(z_test_random_master[idx]), width, label = 'master')\n",
    "    ax.set_ylabel('Confidences')\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_excel(\"ts_fashion_results.xlsx\")  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
