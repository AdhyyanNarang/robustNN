{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0611 14:30:40.103560 4439213504 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/cleverhans/utils_tf.py:341: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "import logging\n",
    "import os, random, time, sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.layers as layers\n",
    "import ipdb\n",
    "import keras\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append('../../../')\n",
    "import feedforward_robust as ffr\n",
    "\n",
    "sys.path.append('../../../utils/')\n",
    "from utils.mnist_corruption import *\n",
    "from utils.utils_models import *\n",
    "from utils.utils_analysis import *\n",
    "from utils.utils_feedforward import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "#Read the counter\n",
    "ctr_file = \"../../counter.txt\"\n",
    "f = open(ctr_file, 'r')\n",
    "counter = f.readline()\n",
    "f.close()\n",
    "\n",
    "counter = 1 + int(counter)\n",
    "f = open(ctr_file,'w')\n",
    "f.write('{}'.format(counter))\n",
    "f.close()\n",
    "logfile = \"../../logs/results_\" + str(counter) + \".log\"\n",
    "\n",
    "logger = logging.getLogger(\"robustness\")\n",
    "logger.setLevel(logging.DEBUG)\n",
    "fh = logging.FileHandler(logfile)\n",
    "fh.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "fh.setFormatter(formatter)\n",
    "logger.addHandler(fh)\n",
    "logger.addHandler(logging.StreamHandler())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Fashion MNIST Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "x_train = x_train/255\n",
    "x_test = x_test/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One hot the labels\n",
    "num_classes = 10\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)                                                                                         \n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_master, y_train_master = x_train[:30000], y_train[:30000]\n",
    "x_train_slave, y_train_slave = x_train[30000:], y_train[30000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Flatten everything\n",
    "x_train_master_flat, input_shape = flatten_mnist(x_train_master) \n",
    "x_train_slave_flat, _ = flatten_mnist(x_train_slave)\n",
    "x_test_flat, _  = flatten_mnist(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configurations\n",
    "eps_train = 0.1                                                                                                                            \n",
    "eps_test = 0.1                                                                                                                             \n",
    "tensorboard_dir = \"../tb/\"                                                                                                                \n",
    "weights_dir = \"../weights/\"                                                                                                               \n",
    "load_weights = False                                                                                                              \n",
    "load_counter = 234                                                                                                            \n",
    "sigma = tf.nn.relu                                                                                                                         \n",
    "epochs, reg, lr = 1, 0.00, 1e-3    \n",
    "#epochs, reg, lr = 30, 0.00, 15e-4                                                                                                          \n",
    "pgd_eta, pgd_num_iter = 1e-2, 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initialized instance variables of the robust model class\n",
      "I0611 14:30:41.150832 4439213504 feedforward_robust.py:32] Initialized instance variables of the robust model class\n",
      "W0611 14:30:41.153137 4439213504 deprecation_wrapper.py:119] From ../../../feedforward_robust.py:36: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "Created placeholders for x and y\n",
      "I0611 14:30:41.161875 4439213504 feedforward_robust.py:40] Created placeholders for x and y\n",
      "W0611 14:30:41.163236 4439213504 deprecation_wrapper.py:119] From ../../../utils/utils_feedforward.py:34: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "W0611 14:30:41.163862 4439213504 deprecation_wrapper.py:119] From ../../../utils/utils_feedforward.py:34: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
      "\n",
      "W0611 14:30:41.164582 4439213504 deprecation_wrapper.py:119] From ../../../utils/utils_feedforward.py:36: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "W0611 14:30:41.180080 4439213504 deprecation_wrapper.py:119] From ../../../utils/utils_feedforward.py:40: The name tf.summary.histogram is deprecated. Please use tf.compat.v1.summary.histogram instead.\n",
      "\n",
      "Created layers and tensor for logits\n",
      "I0611 14:30:41.235514 4439213504 feedforward_robust.py:44] Created layers and tensor for logits\n",
      "Added accuracy computation to the graph\n",
      "I0611 14:30:41.244336 4439213504 feedforward_robust.py:48] Added accuracy computation to the graph\n",
      "W0611 14:30:41.246984 4439213504 deprecation_wrapper.py:119] From ../../../feedforward_robust.py:49: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
      "\n",
      "W0611 14:30:41.249483 4439213504 deprecation.py:323] From ../../../feedforward_robust.py:53: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "Added cross-entropy loss computation to the graph\n",
      "I0611 14:30:41.280559 4439213504 feedforward_robust.py:56] Added cross-entropy loss computation to the graph\n",
      "Model graph was created\n",
      "I0611 14:30:41.281919 4439213504 feedforward_robust.py:62] Model graph was created\n",
      "W0611 14:30:41.288379 4439213504 deprecation_wrapper.py:119] From ../../../feedforward_robust.py:63: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      "W0611 14:30:41.343941 4439213504 deprecation_wrapper.py:119] From ../../../utils/utils_feedforward.py:78: The name tf.linalg.transpose is deprecated. Please use tf.linalg.matrix_transpose instead.\n",
      "\n",
      "W0611 14:30:41.383076 4439213504 deprecation.py:323] From ../../../feedforward_robust.py:739: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Please use tf.global_variables instead.\n",
      "W0611 14:30:41.383865 4439213504 deprecation_wrapper.py:119] From ../../../feedforward_robust.py:740: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created model successfully. Now going to train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0611 14:30:41.814190 4439213504 deprecation.py:323] From /usr/local/lib/python3.7/site-packages/tensorflow/python/util/tf_should_use.py:193: initialize_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.variables_initializer` instead.\n",
      "Epoch: 0001    cost: 0.609549703 \n",
      "I0611 14:30:43.120464 4439213504 feedforward_robust.py:716] Epoch: 0001    cost: 0.609549703 \n",
      "Accuracy on batch: 0.812500\n",
      "I0611 14:30:43.121809 4439213504 feedforward_robust.py:717] Accuracy on batch: 0.812500\n",
      "Optimization Finished!\n",
      "I0611 14:30:43.123045 4439213504 feedforward_robust.py:718] Optimization Finished!\n",
      "Final Train Loss 0.434829\n",
      "I0611 14:30:43.305584 4439213504 feedforward_robust.py:726] Final Train Loss 0.434829\n",
      "Final Train Accuracy 0.843000:\n",
      "I0611 14:30:43.308176 4439213504 feedforward_robust.py:727] Final Train Accuracy 0.843000:\n",
      "Model was trained on benign data\n",
      "I0611 14:30:43.310034 4439213504 feedforward_robust.py:745] Model was trained on benign data\n",
      "Model was evaluated on benign data\n",
      "I0611 14:30:43.368029 4439213504 feedforward_robust.py:642] Model was evaluated on benign data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Regular test accuracy and loss ----\n",
      "(0.48024386, 0.8267)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model is being evaluated on FGSM data\n",
      "I0611 14:30:43.635019 4439213504 feedforward_robust.py:649] Model is being evaluated on FGSM data\n",
      "Model is being evaluated on PGD points generated using 0.010000 learning rate and 50 iterations\n",
      "I0611 14:30:43.694988 4439213504 feedforward_robust.py:651] Model is being evaluated on PGD points generated using 0.010000 learning rate and 50 iterations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----FGSM test accuracy and loss ----\n",
      "(5.2600427, 0.037)\n",
      "iteration: 0\n",
      "loss 0.702678\n",
      "iteration: 1\n",
      "loss 0.992560\n",
      "iteration: 2\n",
      "loss 1.353859\n",
      "iteration: 3\n",
      "loss 1.775042\n",
      "iteration: 4\n",
      "loss 2.236981\n",
      "iteration: 5\n",
      "loss 2.719240\n",
      "iteration: 6\n",
      "loss 3.205202\n",
      "iteration: 7\n",
      "loss 3.681864\n",
      "iteration: 8\n",
      "loss 4.140436\n",
      "iteration: 9\n",
      "loss 4.575137\n",
      "iteration: 10\n",
      "loss 4.981952\n",
      "iteration: 11\n",
      "loss 5.357927\n",
      "iteration: 12\n",
      "loss 5.700738\n",
      "iteration: 13\n",
      "loss 6.009244\n",
      "iteration: 14\n",
      "loss 6.281857\n",
      "iteration: 15\n",
      "loss 6.518266\n",
      "iteration: 16\n",
      "loss 6.717598\n",
      "iteration: 17\n",
      "loss 6.878910\n",
      "iteration: 18\n",
      "loss 7.001528\n",
      "iteration: 19\n",
      "loss 7.084291\n",
      "iteration: 20\n",
      "loss 7.143650\n",
      "iteration: 21\n",
      "loss 7.196340\n",
      "iteration: 22\n",
      "loss 7.242820\n",
      "iteration: 23\n",
      "loss 7.283857\n",
      "iteration: 24\n",
      "loss 7.319435\n",
      "iteration: 25\n",
      "loss 7.351280\n",
      "iteration: 26\n",
      "loss 7.378484\n",
      "iteration: 27\n",
      "loss 7.402395\n",
      "iteration: 28\n",
      "loss 7.422898\n",
      "iteration: 29\n",
      "loss 7.441172\n",
      "iteration: 30\n",
      "loss 7.456722\n",
      "iteration: 31\n",
      "loss 7.470504\n",
      "iteration: 32\n",
      "loss 7.482231\n",
      "iteration: 33\n",
      "loss 7.492923\n",
      "iteration: 34\n",
      "loss 7.501700\n",
      "iteration: 35\n",
      "loss 7.510108\n",
      "iteration: 36\n",
      "loss 7.517172\n",
      "iteration: 37\n",
      "loss 7.523869\n",
      "iteration: 38\n",
      "loss 7.529394\n",
      "iteration: 39\n",
      "loss 7.534830\n",
      "iteration: 40\n",
      "loss 7.539394\n",
      "iteration: 41\n",
      "loss 7.543695\n",
      "iteration: 42\n",
      "loss 7.547630\n",
      "iteration: 43\n",
      "loss 7.551284\n",
      "iteration: 44\n",
      "loss 7.554458\n",
      "iteration: 45\n",
      "loss 7.557376\n",
      "iteration: 46\n",
      "loss 7.560209\n",
      "iteration: 47\n",
      "loss 7.562731\n",
      "iteration: 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This is to confirm that attack does not violate constraints\n",
      "I0611 14:30:48.852742 4439213504 feedforward_robust.py:496] This is to confirm that attack does not violate constraints\n",
      "Should be no more than eps\n",
      "I0611 14:30:48.854115 4439213504 feedforward_robust.py:497] Should be no more than eps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 7.565024\n",
      "iteration: 49\n",
      "loss 7.567389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.10000005352730845\n",
      "I0611 14:30:48.918920 4439213504 feedforward_robust.py:498] 0.10000005352730845\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----PGD test accuracy and loss ----\n",
      "(7.567389, 0.0141)\n"
     ]
    }
   ],
   "source": [
    "#Setup - Dataset stuff\n",
    "tf.reset_default_graph()\n",
    "sess = tf.Session()\n",
    "hidden_sizes = [64, 64, 32]\n",
    "dataset = ((x_train_master_flat, y_train_master), (x_test_flat, y_test))\n",
    "\n",
    "scope_name = \"teacher_student_fashion\"\n",
    "if not load_weights:\n",
    "    with tf.variable_scope(scope_name, reuse = False) as scope:\n",
    "\n",
    "        logdir = tensorboard_dir + str(counter)\n",
    "\n",
    "        #Create model\n",
    "        writer = tf.summary.FileWriter(logdir)\n",
    "        model = ffr.RobustMLP(input_shape, hidden_sizes, num_classes, writer = writer, scope = scope_name, logger = logger, sigma = sigma)\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        print(\"Created model successfully. Now going to train\")\n",
    "    \n",
    "        #Train model\n",
    "        model.fit(sess, x_train_master_flat, y_train_master, training_epochs = epochs, reg_op = reg , lr = lr)\n",
    "        \n",
    "        \"\"\"\n",
    "        #Save weights\n",
    "        weights = tf.trainable_variables()\n",
    "        #weights = model.get_weights()[0] + model.get_weights()[1]\n",
    "        saver = tf.train.Saver(weights)\n",
    "        weights_path = saver.save(sess, weights_dir + \"model_\" + str(counter) + \".ckpt\")\n",
    "        print(\"Saved model at %s\"%weights_path)\n",
    "        \"\"\"\n",
    "        \n",
    "        #Test model - regular, fgsm adv, pgd adv\n",
    "        loss_reg, acc_reg = model.evaluate(sess, x_test_flat, y_test)\n",
    "        print(\"----Regular test accuracy and loss ----\")\n",
    "        print((loss_reg, acc_reg))\n",
    "        \n",
    "        loss_fgsm, acc_fgsm = model.adv_evaluate(sess, x_test_flat, y_test, eps_test, pgd = False)\n",
    "        print(\"----FGSM test accuracy and loss ----\")\n",
    "        print((loss_fgsm, acc_fgsm))\n",
    "        \n",
    "        loss_pgd, acc_pgd = model.adv_evaluate(sess, x_test_flat, y_test, eps_test, pgd = True, eta=pgd_eta, num_iter = pgd_num_iter)\n",
    "        print(\"----PGD test accuracy and loss ----\")\n",
    "        print((loss_pgd , acc_pgd))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate data for slave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model was evaluated on benign data\n",
      "I0611 14:30:49.023163 4439213504 feedforward_robust.py:642] Model was evaluated on benign data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Regular test accuracy and loss ----\n",
      "(0.48024386, 0.8267)\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope(scope_name, reuse = False) as scope:\n",
    "        loss_reg, acc_reg = model.evaluate(sess, x_test_flat, y_test)\n",
    "        print(\"----Regular test accuracy and loss ----\")\n",
    "        print((loss_reg, acc_reg))\n",
    "        \n",
    "        z_train_slave = model.get_prediction(sess, x_train_slave_flat)\n",
    "        z_test_slave = model.get_prediction(sess, x_test_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.31469756,  1.8906322 , -2.1341326 ,  6.425183  , -0.7633205 ,\n",
       "       -5.4411635 , -2.3879952 , -8.553714  , -3.4101493 , -8.622425  ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_train_slave[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train slave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup - Dataset stuff\n",
    "def slave_training():\n",
    "    epochs = 20\n",
    "    lr = 15e-4\n",
    "    tf.reset_default_graph()\n",
    "    sess = tf.Session()\n",
    "    hidden_sizes = [64, 64, 32]\n",
    "    dataset = ((x_train_slave_flat, z_train_slave), (x_test_flat, y_test))\n",
    "\n",
    "    scope_name = \"teacher_student_fashion\"\n",
    "    if not load_weights:\n",
    "        with tf.variable_scope(scope_name, reuse = tf.AUTO_REUSE) as scope:\n",
    "\n",
    "            logdir = tensorboard_dir + str(counter)\n",
    "\n",
    "            #Create model\n",
    "            writer = tf.summary.FileWriter(logdir)\n",
    "            model = ffr.RobustMLP(input_shape, hidden_sizes, num_classes, writer = writer, scope = scope_name, logger = logger, sigma = sigma, classification = False)\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            print(\"Created model successfully. Now going to train\")\n",
    "\n",
    "            #Train model\n",
    "            model.fit(sess, x_train_slave_flat, z_train_slave, training_epochs = epochs, reg_op = reg , lr = lr)\n",
    "\n",
    "            \"\"\"\n",
    "            #Save weights\n",
    "            weights = tf.trainable_variables()\n",
    "            #weights = model.get_weights()[0] + model.get_weights()[1]\n",
    "            saver = tf.train.Saver(weights)\n",
    "            weights_path = saver.save(sess, weights_dir + \"model_\" + str(counter) + \".ckpt\")\n",
    "            print(\"Saved model at %s\"%weights_path)\n",
    "            \"\"\"\n",
    "            loss_real_train, acc_train = model.evaluate(sess, x_train_slave_flat, z_train_slave)\n",
    "\n",
    "            #Test model - regular, fgsm adv, pgd adv\n",
    "                        \n",
    "            loss_class_reg, acc_reg = model.evaluate(sess, x_test_flat, y_test)\n",
    "            print(\"----Regular test loss and accuracy ----\")\n",
    "            print((loss_class_reg, acc_reg))\n",
    "            \n",
    "            loss_real_reg, acc_real_reg = model.evaluate(sess, x_test_flat, z_test_slave)\n",
    "            print(\"----Real test loss and accuracy comparing to teacher ----\")\n",
    "            print((loss_real_reg, acc_real_reg))\n",
    "\n",
    "            loss_fgsm, acc_fgsm = model.adv_evaluate(sess, x_test_flat, y_test, eps_test, pgd = False)\n",
    "            print(\"----FGSM test loss and accuracy ----\")\n",
    "            print((loss_fgsm, acc_fgsm))\n",
    "\n",
    "            loss_pgd, acc_pgd = model.adv_evaluate(sess, x_test_flat, y_test, eps_test, pgd = True, eta=pgd_eta, num_iter = pgd_num_iter)\n",
    "            print(\"----PGD test loss and accuracy ----\")\n",
    "            print((loss_pgd , acc_pgd))\n",
    "            \n",
    "            slave_train_confidences = model.get_prediction(sess, x_train_slave_flat)\n",
    "            slave_test_confidences = model.get_prediction(sess, x_test_flat)\n",
    "            \n",
    "            return loss_real_train, acc_train, loss_real_reg, acc_real_reg, acc_reg, slave_train_confidences, slave_test_confidences\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initialized instance variables of the robust model class\n",
      "I0611 14:42:09.541872 4439213504 feedforward_robust.py:32] Initialized instance variables of the robust model class\n",
      "Created placeholders for x and y\n",
      "I0611 14:42:09.549396 4439213504 feedforward_robust.py:40] Created placeholders for x and y\n",
      "Created layers and tensor for logits\n",
      "I0611 14:42:09.649940 4439213504 feedforward_robust.py:44] Created layers and tensor for logits\n",
      "Added accuracy computation to the graph\n",
      "I0611 14:42:09.656579 4439213504 feedforward_robust.py:48] Added accuracy computation to the graph\n",
      "Added MSE loss computation to the graph\n",
      "I0611 14:42:09.663799 4439213504 feedforward_robust.py:60] Added MSE loss computation to the graph\n",
      "Model graph was created\n",
      "I0611 14:42:09.665156 4439213504 feedforward_robust.py:62] Model graph was created\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created model successfully. Now going to train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 0001    cost: 2.237462381 \n",
      "I0611 14:42:11.227638 4439213504 feedforward_robust.py:716] Epoch: 0001    cost: 2.237462381 \n",
      "Accuracy on batch: 0.906250\n",
      "I0611 14:42:11.228815 4439213504 feedforward_robust.py:717] Accuracy on batch: 0.906250\n",
      "Epoch: 0002    cost: 0.260243641 \n",
      "I0611 14:42:12.117221 4439213504 feedforward_robust.py:716] Epoch: 0002    cost: 0.260243641 \n",
      "Accuracy on batch: 0.906250\n",
      "I0611 14:42:12.119945 4439213504 feedforward_robust.py:717] Accuracy on batch: 0.906250\n",
      "Epoch: 0003    cost: 0.196668937 \n",
      "I0611 14:42:13.158215 4439213504 feedforward_robust.py:716] Epoch: 0003    cost: 0.196668937 \n",
      "Accuracy on batch: 0.937500\n",
      "I0611 14:42:13.159495 4439213504 feedforward_robust.py:717] Accuracy on batch: 0.937500\n",
      "Epoch: 0004    cost: 0.166499397 \n",
      "I0611 14:42:14.130656 4439213504 feedforward_robust.py:716] Epoch: 0004    cost: 0.166499397 \n",
      "Accuracy on batch: 0.937500\n",
      "I0611 14:42:14.131684 4439213504 feedforward_robust.py:717] Accuracy on batch: 0.937500\n",
      "Epoch: 0005    cost: 0.144872741 \n",
      "I0611 14:42:14.988803 4439213504 feedforward_robust.py:716] Epoch: 0005    cost: 0.144872741 \n",
      "Accuracy on batch: 0.937500\n",
      "I0611 14:42:14.990300 4439213504 feedforward_robust.py:717] Accuracy on batch: 0.937500\n",
      "Epoch: 0006    cost: 0.131401379 \n",
      "I0611 14:42:15.924090 4439213504 feedforward_robust.py:716] Epoch: 0006    cost: 0.131401379 \n",
      "Accuracy on batch: 0.968750\n",
      "I0611 14:42:15.925211 4439213504 feedforward_robust.py:717] Accuracy on batch: 0.968750\n",
      "Epoch: 0007    cost: 0.123212515 \n",
      "I0611 14:42:17.228106 4439213504 feedforward_robust.py:716] Epoch: 0007    cost: 0.123212515 \n",
      "Accuracy on batch: 0.968750\n",
      "I0611 14:42:17.229925 4439213504 feedforward_robust.py:717] Accuracy on batch: 0.968750\n",
      "Epoch: 0008    cost: 0.114095935 \n",
      "I0611 14:42:18.899979 4439213504 feedforward_robust.py:716] Epoch: 0008    cost: 0.114095935 \n",
      "Accuracy on batch: 0.937500\n",
      "I0611 14:42:18.904330 4439213504 feedforward_robust.py:717] Accuracy on batch: 0.937500\n",
      "Epoch: 0009    cost: 0.107530084 \n",
      "I0611 14:42:20.181175 4439213504 feedforward_robust.py:716] Epoch: 0009    cost: 0.107530084 \n",
      "Accuracy on batch: 0.968750\n",
      "I0611 14:42:20.182661 4439213504 feedforward_robust.py:717] Accuracy on batch: 0.968750\n",
      "Epoch: 0010    cost: 0.105807922 \n",
      "I0611 14:42:21.224176 4439213504 feedforward_robust.py:716] Epoch: 0010    cost: 0.105807922 \n",
      "Accuracy on batch: 0.968750\n",
      "I0611 14:42:21.226947 4439213504 feedforward_robust.py:717] Accuracy on batch: 0.968750\n",
      "Epoch: 0011    cost: 0.107822478 \n",
      "I0611 14:42:22.526775 4439213504 feedforward_robust.py:716] Epoch: 0011    cost: 0.107822478 \n",
      "Accuracy on batch: 0.968750\n",
      "I0611 14:42:22.530160 4439213504 feedforward_robust.py:717] Accuracy on batch: 0.968750\n",
      "Epoch: 0012    cost: 0.098560397 \n",
      "I0611 14:42:23.903589 4439213504 feedforward_robust.py:716] Epoch: 0012    cost: 0.098560397 \n",
      "Accuracy on batch: 0.968750\n",
      "I0611 14:42:23.904816 4439213504 feedforward_robust.py:717] Accuracy on batch: 0.968750\n",
      "Epoch: 0013    cost: 0.089841248 \n",
      "I0611 14:42:25.096383 4439213504 feedforward_robust.py:716] Epoch: 0013    cost: 0.089841248 \n",
      "Accuracy on batch: 0.968750\n",
      "I0611 14:42:25.100462 4439213504 feedforward_robust.py:717] Accuracy on batch: 0.968750\n",
      "Epoch: 0014    cost: 0.086042099 \n",
      "I0611 14:42:26.340617 4439213504 feedforward_robust.py:716] Epoch: 0014    cost: 0.086042099 \n",
      "Accuracy on batch: 0.968750\n",
      "I0611 14:42:26.342374 4439213504 feedforward_robust.py:717] Accuracy on batch: 0.968750\n",
      "Epoch: 0015    cost: 0.086388654 \n",
      "I0611 14:42:27.292494 4439213504 feedforward_robust.py:716] Epoch: 0015    cost: 0.086388654 \n",
      "Accuracy on batch: 0.937500\n",
      "I0611 14:42:27.293791 4439213504 feedforward_robust.py:717] Accuracy on batch: 0.937500\n",
      "Epoch: 0016    cost: 0.086000988 \n",
      "I0611 14:42:28.315948 4439213504 feedforward_robust.py:716] Epoch: 0016    cost: 0.086000988 \n",
      "Accuracy on batch: 0.937500\n",
      "I0611 14:42:28.327090 4439213504 feedforward_robust.py:717] Accuracy on batch: 0.937500\n",
      "Epoch: 0017    cost: 0.085526110 \n",
      "I0611 14:42:29.394461 4439213504 feedforward_robust.py:716] Epoch: 0017    cost: 0.085526110 \n",
      "Accuracy on batch: 0.937500\n",
      "I0611 14:42:29.396086 4439213504 feedforward_robust.py:717] Accuracy on batch: 0.937500\n",
      "Epoch: 0018    cost: 0.082724047 \n",
      "I0611 14:42:30.356534 4439213504 feedforward_robust.py:716] Epoch: 0018    cost: 0.082724047 \n",
      "Accuracy on batch: 0.937500\n",
      "I0611 14:42:30.357707 4439213504 feedforward_robust.py:717] Accuracy on batch: 0.937500\n",
      "Epoch: 0019    cost: 0.077071215 \n",
      "I0611 14:42:31.341720 4439213504 feedforward_robust.py:716] Epoch: 0019    cost: 0.077071215 \n",
      "Accuracy on batch: 0.937500\n",
      "I0611 14:42:31.343008 4439213504 feedforward_robust.py:717] Accuracy on batch: 0.937500\n",
      "Epoch: 0020    cost: 0.073580598 \n",
      "I0611 14:42:32.456643 4439213504 feedforward_robust.py:716] Epoch: 0020    cost: 0.073580598 \n",
      "Accuracy on batch: 0.968750\n",
      "I0611 14:42:32.458484 4439213504 feedforward_robust.py:717] Accuracy on batch: 0.968750\n",
      "Optimization Finished!\n",
      "I0611 14:42:32.459647 4439213504 feedforward_robust.py:718] Optimization Finished!\n",
      "Final Train Loss 0.081613\n",
      "I0611 14:42:32.600310 4439213504 feedforward_robust.py:726] Final Train Loss 0.081613\n",
      "Final Train Accuracy 0.977200:\n",
      "I0611 14:42:32.601413 4439213504 feedforward_robust.py:727] Final Train Accuracy 0.977200:\n",
      "Model was trained on benign data\n",
      "I0611 14:42:32.603189 4439213504 feedforward_robust.py:745] Model was trained on benign data\n",
      "Model was evaluated on benign data\n",
      "I0611 14:42:32.668866 4439213504 feedforward_robust.py:642] Model was evaluated on benign data\n",
      "Model was evaluated on benign data\n",
      "I0611 14:42:32.716177 4439213504 feedforward_robust.py:642] Model was evaluated on benign data\n",
      "Model was evaluated on benign data\n",
      "I0611 14:42:32.744890 4439213504 feedforward_robust.py:642] Model was evaluated on benign data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Regular test loss and accuracy ----\n",
      "(65.91971, 0.8262)\n",
      "----Real test loss and accuracy comparing to teacher ----\n",
      "(0.09702799, 0.9754)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model is being evaluated on FGSM data\n",
      "I0611 14:42:32.918411 4439213504 feedforward_robust.py:649] Model is being evaluated on FGSM data\n",
      "Model is being evaluated on PGD points generated using 0.010000 learning rate and 50 iterations\n",
      "I0611 14:42:32.936987 4439213504 feedforward_robust.py:651] Model is being evaluated on PGD points generated using 0.010000 learning rate and 50 iterations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----FGSM test loss and accuracy ----\n",
      "(202.65367, 0.6256)\n",
      "iteration: 0\n",
      "loss 0.737341\n",
      "iteration: 1\n",
      "loss 1.070574\n",
      "iteration: 2\n",
      "loss 1.491013\n",
      "iteration: 3\n",
      "loss 1.980300\n",
      "iteration: 4\n",
      "loss 2.514758\n",
      "iteration: 5\n",
      "loss 3.068840\n",
      "iteration: 6\n",
      "loss 3.622521\n",
      "iteration: 7\n",
      "loss 4.161366\n",
      "iteration: 8\n",
      "loss 4.676623\n",
      "iteration: 9\n",
      "loss 5.162650\n",
      "iteration: 10\n",
      "loss 5.615674\n",
      "iteration: 11\n",
      "loss 6.033039\n",
      "iteration: 12\n",
      "loss 6.412905\n",
      "iteration: 13\n",
      "loss 6.754102\n",
      "iteration: 14\n",
      "loss 7.055805\n",
      "iteration: 15\n",
      "loss 7.316837\n",
      "iteration: 16\n",
      "loss 7.537161\n",
      "iteration: 17\n",
      "loss 7.715266\n",
      "iteration: 18\n",
      "loss 7.850503\n",
      "iteration: 19\n",
      "loss 7.942114\n",
      "iteration: 20\n",
      "loss 8.008368\n",
      "iteration: 21\n",
      "loss 8.067415\n",
      "iteration: 22\n",
      "loss 8.119392\n",
      "iteration: 23\n",
      "loss 8.165595\n",
      "iteration: 24\n",
      "loss 8.206116\n",
      "iteration: 25\n",
      "loss 8.241647\n",
      "iteration: 26\n",
      "loss 8.272553\n",
      "iteration: 27\n",
      "loss 8.299675\n",
      "iteration: 28\n",
      "loss 8.323347\n",
      "iteration: 29\n",
      "loss 8.344050\n",
      "iteration: 30\n",
      "loss 8.362108\n",
      "iteration: 31\n",
      "loss 8.378006\n",
      "iteration: 32\n",
      "loss 8.392079\n",
      "iteration: 33\n",
      "loss 8.404270\n",
      "iteration: 34\n",
      "loss 8.415139\n",
      "iteration: 35\n",
      "loss 8.424539\n",
      "iteration: 36\n",
      "loss 8.432908\n",
      "iteration: 37\n",
      "loss 8.440131\n",
      "iteration: 38\n",
      "loss 8.446651\n",
      "iteration: 39\n",
      "loss 8.452807\n",
      "iteration: 40\n",
      "loss 8.458092\n",
      "iteration: 41\n",
      "loss 8.462940\n",
      "iteration: 42\n",
      "loss 8.467514\n",
      "iteration: 43\n",
      "loss 8.471809\n",
      "iteration: 44\n",
      "loss 8.475446\n",
      "iteration: 45\n",
      "loss 8.479226\n",
      "iteration: 46\n",
      "loss 8.482246\n",
      "iteration: 47\n",
      "loss 8.485579\n",
      "iteration: 48\n",
      "loss 8.488000\n",
      "iteration: 49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This is to confirm that attack does not violate constraints\n",
      "I0611 14:42:37.870707 4439213504 feedforward_robust.py:496] This is to confirm that attack does not violate constraints\n",
      "Should be no more than eps\n",
      "I0611 14:42:37.872554 4439213504 feedforward_robust.py:497] Should be no more than eps\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 8.490906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.10000005352730845\n",
      "I0611 14:42:37.961200 4439213504 feedforward_robust.py:498] 0.10000005352730845\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----PGD test loss and accuracy ----\n",
      "(111.69142, 0.0108)\n"
     ]
    }
   ],
   "source": [
    "tup = slave_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-3f786850e387>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'a' is not defined"
     ]
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(tup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df['mse on z_train'] = []\n",
    "df['acc on z_train'] = []\n",
    "df['mse on z_test'] = []\n",
    "df['acc on z_test'] = []\n",
    "df['acc on y_test'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_confidences = []\n",
    "test_confidences = []\n",
    "for i in range(3):\n",
    "    loss_real_train, acc_train, loss_real_reg, acc_real_reg, acc_reg, slave_train_confidences, slave_test_confidences = slave_training()\n",
    "    df.loc[i] = [loss_real_train, acc_train, loss_real_reg, acc_real_reg, acc_reg]\n",
    "    train_confidences.append(slave_train_confidences)\n",
    "    test_confidences.append(slave_test_confidences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_excel(\"ts_fashion_results.xlsx\")  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
