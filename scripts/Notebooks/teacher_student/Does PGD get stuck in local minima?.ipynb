{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0716 15:31:41.394906 4654771648 deprecation_wrapper.py:119] From /usr/local/lib/python3.7/site-packages/cleverhans/utils_tf.py:341: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "import logging\n",
    "import os, random, time, sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.layers as layers\n",
    "import ipdb\n",
    "import keras\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append('../../../')\n",
    "import feedforward_robust as ffr\n",
    "\n",
    "sys.path.append('../../../utils/')\n",
    "from utils.mnist_corruption import *\n",
    "from utils.utils_models import *\n",
    "from utils.utils_analysis import *\n",
    "from utils.utils_feedforward import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "#Read the counter\n",
    "ctr_file = \"../../counter.txt\"\n",
    "f = open(ctr_file, 'r')\n",
    "counter = f.readline()\n",
    "f.close()\n",
    "\n",
    "counter = 1 + int(counter)\n",
    "f = open(ctr_file,'w')\n",
    "f.write('{}'.format(counter))\n",
    "f.close()\n",
    "logfile = \"../../logs/results_\" + str(counter) + \".log\"\n",
    "\n",
    "logger = logging.getLogger(\"robustness\")\n",
    "logger.setLevel(logging.DEBUG)\n",
    "fh = logging.FileHandler(logfile)\n",
    "fh.setLevel(logging.INFO)\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "fh.setFormatter(formatter)\n",
    "logger.addHandler(fh)\n",
    "logger.addHandler(logging.StreamHandler())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Fashion MNIST Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "x_train = x_train/255\n",
    "x_test = x_test/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One hot the labels\n",
    "num_classes = 10\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)                                                                                         \n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_master, y_train_master = x_train[:30000], y_train[:30000]\n",
    "x_train_slave, y_train_slave = x_train[30000:], y_train[30000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nx_train_master = x_train_master[:, :21, :21]\\nx_test = x_test[:, :21, :21]\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "x_train_master = x_train_master[:, :21, :21]\n",
    "x_test = x_test[:, :21, :21]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Flatten everything\n",
    "x_train_master_flat, input_shape = flatten_mnist(x_train_master) \n",
    "x_train_slave_flat, _ = flatten_mnist(x_train_slave)\n",
    "x_test_flat, _  = flatten_mnist(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configurations\n",
    "eps_train = 0.1                                                                                                                            \n",
    "eps_test = 1.0                                                                                                                            \n",
    "tensorboard_dir = \"../tb/\"                                                                                                                \n",
    "weights_dir = \"../weights/\"                                                                                                               \n",
    "load_weights = False                                                                                                              \n",
    "load_counter = 234                                                                                                            \n",
    "sigma = tf.nn.relu                                                                                                                         \n",
    "epochs, reg, lr = 3, 0.00, 1e-3    \n",
    "#epochs, reg, lr = 30, 0.00, 15e-4                                                                                                          \n",
    "pgd_eta, pgd_num_iter = 5000, 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initialized instance variables of the robust model class\n",
      "I0716 15:52:05.044623 4654771648 feedforward_robust.py:32] Initialized instance variables of the robust model class\n",
      "Created placeholders for x and y\n",
      "I0716 15:52:05.054842 4654771648 feedforward_robust.py:40] Created placeholders for x and y\n",
      "Created layers and tensor for logits\n",
      "I0716 15:52:05.139797 4654771648 feedforward_robust.py:44] Created layers and tensor for logits\n",
      "Added accuracy computation to the graph\n",
      "I0716 15:52:05.151144 4654771648 feedforward_robust.py:48] Added accuracy computation to the graph\n",
      "Added cross-entropy loss computation to the graph\n",
      "I0716 15:52:05.186601 4654771648 feedforward_robust.py:56] Added cross-entropy loss computation to the graph\n",
      "Model graph was created\n",
      "I0716 15:52:05.187947 4654771648 feedforward_robust.py:62] Model graph was created\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created model successfully. Now going to train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 0001    cost: 0.598623769 \n",
      "I0716 15:52:07.033156 4654771648 feedforward_robust.py:763] Epoch: 0001    cost: 0.598623769 \n",
      "Accuracy on batch: 0.781250\n",
      "I0716 15:52:07.034469 4654771648 feedforward_robust.py:764] Accuracy on batch: 0.781250\n",
      "Epoch: 0002    cost: 0.426670679 \n",
      "I0716 15:52:08.159101 4654771648 feedforward_robust.py:763] Epoch: 0002    cost: 0.426670679 \n",
      "Accuracy on batch: 0.812500\n",
      "I0716 15:52:08.160121 4654771648 feedforward_robust.py:764] Accuracy on batch: 0.812500\n",
      "Epoch: 0003    cost: 0.382036593 \n",
      "I0716 15:52:09.318949 4654771648 feedforward_robust.py:763] Epoch: 0003    cost: 0.382036593 \n",
      "Accuracy on batch: 0.843750\n",
      "I0716 15:52:09.319987 4654771648 feedforward_robust.py:764] Accuracy on batch: 0.843750\n",
      "Optimization Finished!\n",
      "I0716 15:52:09.321039 4654771648 feedforward_robust.py:765] Optimization Finished!\n",
      "Final Train Loss 0.349094\n",
      "I0716 15:52:09.484955 4654771648 feedforward_robust.py:773] Final Train Loss 0.349094\n",
      "Final Train Accuracy 0.871300:\n",
      "I0716 15:52:09.486498 4654771648 feedforward_robust.py:774] Final Train Accuracy 0.871300:\n",
      "Model was trained on benign data\n",
      "I0716 15:52:09.493379 4654771648 feedforward_robust.py:796] Model was trained on benign data\n",
      "Model was evaluated on benign data\n",
      "I0716 15:52:09.544981 4654771648 feedforward_robust.py:675] Model was evaluated on benign data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Regular test accuracy and loss ----\n",
      "(0.41703442, 0.8532)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model is being evaluated on FGSM data\n",
      "I0716 15:52:09.792414 4654771648 feedforward_robust.py:682] Model is being evaluated on FGSM data\n",
      "Model is being evaluated on PGD points generated using 5000.000000 learning rate and 500 iterations\n",
      "I0716 15:52:09.886036 4654771648 feedforward_robust.py:684] Model is being evaluated on PGD points generated using 5000.000000 learning rate and 500 iterations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----FGSM test accuracy and loss ----\n",
      "(26.414831, 1e-04)\n",
      "iteration: 0\n",
      "loss 5.778622\n",
      "iteration: 20\n",
      "loss 62.206608\n",
      "iteration: 40\n",
      "loss 64.861809\n",
      "iteration: 60\n",
      "loss 65.614998\n",
      "iteration: 80\n",
      "loss 66.039391\n",
      "iteration: 100\n",
      "loss 66.296066\n",
      "iteration: 120\n",
      "loss 66.473572\n",
      "iteration: 140\n",
      "loss 66.620018\n",
      "iteration: 160\n",
      "loss 66.743759\n",
      "iteration: 180\n",
      "loss 66.868004\n",
      "iteration: 200\n",
      "loss 66.968971\n",
      "iteration: 220\n",
      "loss 67.024162\n",
      "iteration: 240\n",
      "loss 67.089355\n",
      "iteration: 260\n",
      "loss 67.149811\n",
      "iteration: 280\n",
      "loss 67.217201\n",
      "iteration: 300\n",
      "loss 67.279816\n",
      "iteration: 320\n",
      "loss 67.335037\n",
      "iteration: 340\n",
      "loss 67.378349\n",
      "iteration: 360\n",
      "loss 67.412270\n",
      "iteration: 380\n",
      "loss 67.448700\n",
      "iteration: 400\n",
      "loss 67.490921\n",
      "iteration: 420\n",
      "loss 67.533096\n",
      "iteration: 440\n",
      "loss 67.563850\n",
      "iteration: 460\n",
      "loss 67.593697\n",
      "iteration: 480\n",
      "loss 67.628471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This is to confirm that attack does not violate constraints\n",
      "I0716 15:53:01.570985 4654771648 feedforward_robust.py:529] This is to confirm that attack does not violate constraints\n",
      "Should be no more than eps\n",
      "I0716 15:53:01.572118 4654771648 feedforward_robust.py:530] Should be no more than eps\n",
      "1.0\n",
      "I0716 15:53:01.622884 4654771648 feedforward_robust.py:531] 1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----PGD test accuracy and loss ----\n",
      "(67.6588, 0.0225)\n"
     ]
    }
   ],
   "source": [
    "#Setup - Dataset stuff\n",
    "tf.reset_default_graph()\n",
    "sess = tf.Session()\n",
    "hidden_sizes = [64, 64, 32]\n",
    "dataset = ((x_train_master_flat, y_train_master), (x_test_flat, y_test))\n",
    "\n",
    "scope_name = \"teacher_student_fashion\"\n",
    "if not load_weights:\n",
    "    with tf.variable_scope(scope_name, reuse = False) as scope:\n",
    "\n",
    "        logdir = tensorboard_dir + str(counter)\n",
    "\n",
    "        #Create model\n",
    "        writer = tf.summary.FileWriter(logdir)\n",
    "        model = ffr.RobustMLP(input_shape, hidden_sizes, num_classes, writer = writer, scope = scope_name, logger = logger, sigma = sigma)\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        print(\"Created model successfully. Now going to train\")\n",
    "    \n",
    "        #Train model\n",
    "        model.fit(sess, x_train_master_flat, y_train_master, training_epochs = epochs, reg_op = reg , lr = lr)\n",
    "        \n",
    "        \"\"\"\n",
    "        #Save weights\n",
    "        weights = tf.trainable_variables()\n",
    "        #weights = model.get_weights()[0] + model.get_weights()[1]\n",
    "        saver = tf.train.Saver(weights)\n",
    "        weights_path = saver.save(sess, weights_dir + \"model_\" + str(counter) + \".ckpt\")\n",
    "        print(\"Saved model at %s\"%weights_path)\n",
    "        \"\"\"\n",
    "        \n",
    "        #Test model - regular, fgsm adv, pgd adv\n",
    "        loss_reg, acc_reg = model.evaluate(sess, x_test_flat, y_test)\n",
    "        print(\"----Regular test accuracy and loss ----\")\n",
    "        print((loss_reg, acc_reg))\n",
    "        \n",
    "        loss_fgsm, acc_fgsm, _ = model.adv_evaluate(sess, x_test_flat, y_test, eps_test, pgd = False)\n",
    "        print(\"----FGSM test accuracy and loss ----\")\n",
    "        print((loss_fgsm, acc_fgsm))\n",
    "        \n",
    "        loss_pgd, acc_pgd, deltas = model.adv_evaluate(sess, x_test_flat, y_test, eps_test, pgd = True, eta=pgd_eta, num_iter = pgd_num_iter)\n",
    "        print(\"----PGD test accuracy and loss ----\")\n",
    "        print((loss_pgd , acc_pgd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "save_string = \"/Users/adhyyan/Desktop/delta_hist_oneV2.pdf\"\n",
    "plt.hist(deltas.flatten(), 30)\n",
    "plt.savefig(save_string, format = 'pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pgd = x_test_flat + deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_pgd_2d = x_pgd.reshape((10000, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(x_pgd_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.01176471,\n",
       "        0.00392157, 0.        , 0.        , 0.02745098, 0.        ,\n",
       "        0.14509804, 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.00392157, 0.00784314,\n",
       "        0.        , 0.10588235, 0.32941176, 0.04313725, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.46666667, 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.00392157, 0.        ,\n",
       "        0.        , 0.34509804, 0.56078431, 0.43137255, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.08627451, 0.36470588,\n",
       "        0.41568627, 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.01568627, 0.        ,\n",
       "        0.20784314, 0.50588235, 0.47058824, 0.57647059, 0.68627451,\n",
       "        0.61568627, 0.65098039, 0.52941176, 0.60392157, 0.65882353,\n",
       "        0.54901961, 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.00784314, 0.        , 0.04313725,\n",
       "        0.5372549 , 0.50980392, 0.50196078, 0.62745098, 0.69019608,\n",
       "        0.62352941, 0.65490196, 0.69803922, 0.58431373, 0.59215686,\n",
       "        0.56470588, 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.00392157, 0.        , 0.00784314, 0.00392157,\n",
       "        0.        , 0.01176471, 0.        , 0.        , 0.45098039,\n",
       "        0.44705882, 0.41568627, 0.5372549 , 0.65882353, 0.6       ,\n",
       "        0.61176471, 0.64705882, 0.65490196, 0.56078431, 0.61568627,\n",
       "        0.61960784, 0.04313725, 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.00392157,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.01176471, 0.        , 0.        , 0.34901961, 0.54509804,\n",
       "        0.35294118, 0.36862745, 0.6       , 0.58431373, 0.51372549,\n",
       "        0.59215686, 0.6627451 , 0.6745098 , 0.56078431, 0.62352941,\n",
       "        0.6627451 , 0.18823529, 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.00784314, 0.01568627, 0.00392157, 0.        ,\n",
       "        0.        , 0.        , 0.38431373, 0.53333333, 0.43137255,\n",
       "        0.42745098, 0.43137255, 0.63529412, 0.52941176, 0.56470588,\n",
       "        0.58431373, 0.62352941, 0.65490196, 0.56470588, 0.61960784,\n",
       "        0.6627451 , 0.46666667, 0.        ],\n",
       "       [0.        , 0.        , 0.00784314, 0.00784314, 0.00392157,\n",
       "        0.00784314, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.10196078, 0.42352941, 0.45882353, 0.38823529, 0.43529412,\n",
       "        0.45882353, 0.53333333, 0.61176471, 0.5254902 , 0.60392157,\n",
       "        0.60392157, 0.61176471, 0.62745098, 0.55294118, 0.57647059,\n",
       "        0.61176471, 0.69803922, 0.        ],\n",
       "       [0.01176471, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.08235294, 0.20784314, 0.36078431,\n",
       "        0.45882353, 0.43529412, 0.40392157, 0.45098039, 0.50588235,\n",
       "        0.5254902 , 0.56078431, 0.60392157, 0.64705882, 0.66666667,\n",
       "        0.60392157, 0.59215686, 0.60392157, 0.56078431, 0.54117647,\n",
       "        0.58823529, 0.64705882, 0.16862745],\n",
       "       [0.        , 0.        , 0.09019608, 0.21176471, 0.25490196,\n",
       "        0.29803922, 0.33333333, 0.4627451 , 0.50196078, 0.48235294,\n",
       "        0.43529412, 0.44313725, 0.4627451 , 0.49803922, 0.49019608,\n",
       "        0.54509804, 0.52156863, 0.53333333, 0.62745098, 0.54901961,\n",
       "        0.60784314, 0.63137255, 0.56470588, 0.60784314, 0.6745098 ,\n",
       "        0.63137255, 0.74117647, 0.24313725],\n",
       "       [0.        , 0.26666667, 0.36862745, 0.35294118, 0.43529412,\n",
       "        0.44705882, 0.43529412, 0.44705882, 0.45098039, 0.49803922,\n",
       "        0.52941176, 0.53333333, 0.56078431, 0.49411765, 0.49803922,\n",
       "        0.59215686, 0.60392157, 0.56078431, 0.58039216, 0.49019608,\n",
       "        0.63529412, 0.63529412, 0.56470588, 0.54117647, 0.6       ,\n",
       "        0.63529412, 0.76862745, 0.22745098],\n",
       "       [0.2745098 , 0.6627451 , 0.50588235, 0.40784314, 0.38431373,\n",
       "        0.39215686, 0.36862745, 0.38039216, 0.38431373, 0.4       ,\n",
       "        0.42352941, 0.41568627, 0.46666667, 0.47058824, 0.50588235,\n",
       "        0.58431373, 0.61176471, 0.65490196, 0.74509804, 0.74509804,\n",
       "        0.76862745, 0.77647059, 0.77647059, 0.73333333, 0.77254902,\n",
       "        0.74117647, 0.72156863, 0.14117647],\n",
       "       [0.0627451 , 0.49411765, 0.67058824, 0.7372549 , 0.7372549 ,\n",
       "        0.72156863, 0.67058824, 0.6       , 0.52941176, 0.47058824,\n",
       "        0.49411765, 0.49803922, 0.57254902, 0.7254902 , 0.76470588,\n",
       "        0.81960784, 0.81568627, 1.        , 0.81960784, 0.69411765,\n",
       "        0.96078431, 0.98823529, 0.98431373, 0.98431373, 0.96862745,\n",
       "        0.8627451 , 0.80784314, 0.19215686],\n",
       "       [0.        , 0.        , 0.        , 0.04705882, 0.2627451 ,\n",
       "        0.41568627, 0.64313725, 0.7254902 , 0.78039216, 0.82352941,\n",
       "        0.82745098, 0.82352941, 0.81568627, 0.74509804, 0.58823529,\n",
       "        0.32156863, 0.03137255, 0.        , 0.        , 0.        ,\n",
       "        0.69803922, 0.81568627, 0.7372549 , 0.68627451, 0.63529412,\n",
       "        0.61960784, 0.59215686, 0.04313725],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "save_string = \"/Users/adhyyan/Desktop/shoe_original_maxV2.pdf\"\n",
    "plt.imshow(x_test[9], cmap = 'Greys')\n",
    "plt.savefig(save_string, format = 'pdf')\n",
    "plt.show()\n",
    "\n",
    "save_string = \"/Users/adhyyan/Desktop/shoe_perturbed_maxV2.pdf\"\n",
    "plt.imshow(x_pgd_2d[9], cmap = 'Greys')\n",
    "plt.savefig(save_string, format = 'pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate data for slave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model was evaluated on benign data\n",
      "I0716 15:32:00.077793 4654771648 feedforward_robust.py:675] Model was evaluated on benign data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Regular test accuracy and loss ----\n",
      "(0.48954064, 0.8214)\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope(scope_name, reuse = False) as scope:\n",
    "        loss_reg, acc_reg = model.evaluate(sess, x_test_flat, y_test)\n",
    "        print(\"----Regular test accuracy and loss ----\")\n",
    "        print((loss_reg, acc_reg))\n",
    "        \n",
    "        z_train_slave = model.get_prediction(sess, x_train_slave_flat)\n",
    "        z_test_slave = model.get_prediction(sess, x_test_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.7228895,  5.857828 , -1.2812027,  8.737827 ,  2.7070222,\n",
       "       -7.9261427, -0.3017945, -6.3297834, -1.1928556, -2.648211 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z_train_slave[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train slave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup - Dataset stuff\n",
    "def slave_training():\n",
    "    epochs = 20\n",
    "    lr = 15e-4\n",
    "    tf.reset_default_graph()\n",
    "    sess = tf.Session()\n",
    "    hidden_sizes = [64, 64, 32]\n",
    "    dataset = ((x_train_slave_flat, z_train_slave), (x_test_flat, y_test))\n",
    "\n",
    "    scope_name = \"teacher_student_fashion\"\n",
    "    if not load_weights:\n",
    "        with tf.variable_scope(scope_name, reuse = tf.AUTO_REUSE) as scope:\n",
    "\n",
    "            logdir = tensorboard_dir + str(counter)\n",
    "\n",
    "            #Create model\n",
    "            writer = tf.summary.FileWriter(logdir)\n",
    "            model = ffr.RobustMLP(input_shape, hidden_sizes, num_classes, writer = writer, scope = scope_name, logger = logger, sigma = sigma, classification = False)\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            print(\"Created model successfully. Now going to train\")\n",
    "\n",
    "            #Train model\n",
    "            model.fit(sess, x_train_slave_flat, z_train_slave, training_epochs = epochs, reg_op = reg , lr = lr)\n",
    "\n",
    "            \"\"\"\n",
    "            #Save weights\n",
    "            weights = tf.trainable_variables()\n",
    "            #weights = model.get_weights()[0] + model.get_weights()[1]\n",
    "            saver = tf.train.Saver(weights)\n",
    "            weights_path = saver.save(sess, weights_dir + \"model_\" + str(counter) + \".ckpt\")\n",
    "            print(\"Saved model at %s\"%weights_path)\n",
    "            \"\"\"\n",
    "            loss_real_train, acc_train = model.evaluate(sess, x_train_slave_flat, z_train_slave)\n",
    "\n",
    "            #Test model - regular, fgsm adv, pgd adv\n",
    "                        \n",
    "            loss_class_reg, acc_reg = model.evaluate(sess, x_test_flat, y_test)\n",
    "            print(\"----Regular test loss and accuracy ----\")\n",
    "            print((loss_class_reg, acc_reg))\n",
    "            \n",
    "            loss_real_reg, acc_real_reg = model.evaluate(sess, x_test_flat, z_test_slave)\n",
    "            print(\"----Real test loss and accuracy comparing to teacher ----\")\n",
    "            print((loss_real_reg, acc_real_reg))\n",
    "\n",
    "            loss_fgsm, acc_fgsm = model.adv_evaluate(sess, x_test_flat, y_test, eps_test, pgd = False)\n",
    "            print(\"----FGSM test loss and accuracy ----\")\n",
    "            print((loss_fgsm, acc_fgsm))\n",
    "\n",
    "            loss_pgd, acc_pgd = model.adv_evaluate(sess, x_test_flat, y_test, eps_test, pgd = True, eta=pgd_eta, num_iter = pgd_num_iter)\n",
    "            print(\"----PGD test loss and accuracy ----\")\n",
    "            print((loss_pgd , acc_pgd))\n",
    "            \n",
    "            slave_train_confidences = model.get_prediction(sess, x_train_slave_flat)\n",
    "            slave_test_confidences = model.get_prediction(sess, x_test_flat)\n",
    "            \n",
    "            return loss_real_train, acc_train, loss_real_reg, acc_real_reg, acc_reg, slave_train_confidences, slave_test_confidences\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initialized instance variables of the robust model class\n",
      "I0716 15:32:00.509573 4654771648 feedforward_robust.py:32] Initialized instance variables of the robust model class\n",
      "Created placeholders for x and y\n",
      "I0716 15:32:00.520034 4654771648 feedforward_robust.py:40] Created placeholders for x and y\n",
      "Created layers and tensor for logits\n",
      "I0716 15:32:00.597544 4654771648 feedforward_robust.py:44] Created layers and tensor for logits\n",
      "Added accuracy computation to the graph\n",
      "I0716 15:32:00.606265 4654771648 feedforward_robust.py:48] Added accuracy computation to the graph\n",
      "Added MSE loss computation to the graph\n",
      "I0716 15:32:00.631170 4654771648 feedforward_robust.py:60] Added MSE loss computation to the graph\n",
      "Model graph was created\n",
      "I0716 15:32:00.636001 4654771648 feedforward_robust.py:62] Model graph was created\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created model successfully. Now going to train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 0001    cost: 1.983510282 \n",
      "I0716 15:32:02.383177 4654771648 feedforward_robust.py:763] Epoch: 0001    cost: 1.983510282 \n",
      "Accuracy on batch: 1.000000\n",
      "I0716 15:32:02.384288 4654771648 feedforward_robust.py:764] Accuracy on batch: 1.000000\n",
      "Epoch: 0002    cost: 0.275234095 \n",
      "I0716 15:32:04.087764 4654771648 feedforward_robust.py:763] Epoch: 0002    cost: 0.275234095 \n",
      "Accuracy on batch: 1.000000\n",
      "I0716 15:32:04.091649 4654771648 feedforward_robust.py:764] Accuracy on batch: 1.000000\n",
      "Epoch: 0003    cost: 0.214438481 \n",
      "I0716 15:32:05.208437 4654771648 feedforward_robust.py:763] Epoch: 0003    cost: 0.214438481 \n",
      "Accuracy on batch: 1.000000\n",
      "I0716 15:32:05.209533 4654771648 feedforward_robust.py:764] Accuracy on batch: 1.000000\n",
      "Epoch: 0004    cost: 0.183780558 \n",
      "I0716 15:32:06.316774 4654771648 feedforward_robust.py:763] Epoch: 0004    cost: 0.183780558 \n",
      "Accuracy on batch: 1.000000\n",
      "I0716 15:32:06.317914 4654771648 feedforward_robust.py:764] Accuracy on batch: 1.000000\n",
      "Epoch: 0005    cost: 0.162286543 \n",
      "I0716 15:32:07.326483 4654771648 feedforward_robust.py:763] Epoch: 0005    cost: 0.162286543 \n",
      "Accuracy on batch: 1.000000\n",
      "I0716 15:32:07.327564 4654771648 feedforward_robust.py:764] Accuracy on batch: 1.000000\n",
      "Epoch: 0006    cost: 0.147813435 \n",
      "I0716 15:32:08.507227 4654771648 feedforward_robust.py:763] Epoch: 0006    cost: 0.147813435 \n",
      "Accuracy on batch: 1.000000\n",
      "I0716 15:32:08.509019 4654771648 feedforward_robust.py:764] Accuracy on batch: 1.000000\n",
      "Epoch: 0007    cost: 0.139576004 \n",
      "I0716 15:32:10.052072 4654771648 feedforward_robust.py:763] Epoch: 0007    cost: 0.139576004 \n",
      "Accuracy on batch: 1.000000\n",
      "I0716 15:32:10.056635 4654771648 feedforward_robust.py:764] Accuracy on batch: 1.000000\n",
      "Epoch: 0008    cost: 0.130979328 \n",
      "I0716 15:32:11.241894 4654771648 feedforward_robust.py:763] Epoch: 0008    cost: 0.130979328 \n",
      "Accuracy on batch: 1.000000\n",
      "I0716 15:32:11.242993 4654771648 feedforward_robust.py:764] Accuracy on batch: 1.000000\n",
      "Epoch: 0009    cost: 0.124608541 \n",
      "I0716 15:32:12.376603 4654771648 feedforward_robust.py:763] Epoch: 0009    cost: 0.124608541 \n",
      "Accuracy on batch: 1.000000\n",
      "I0716 15:32:12.377676 4654771648 feedforward_robust.py:764] Accuracy on batch: 1.000000\n",
      "Epoch: 0010    cost: 0.118489663 \n",
      "I0716 15:32:13.493271 4654771648 feedforward_robust.py:763] Epoch: 0010    cost: 0.118489663 \n",
      "Accuracy on batch: 1.000000\n",
      "I0716 15:32:13.513088 4654771648 feedforward_robust.py:764] Accuracy on batch: 1.000000\n",
      "Epoch: 0011    cost: 0.115103373 \n",
      "I0716 15:32:14.595108 4654771648 feedforward_robust.py:763] Epoch: 0011    cost: 0.115103373 \n",
      "Accuracy on batch: 1.000000\n",
      "I0716 15:32:14.596374 4654771648 feedforward_robust.py:764] Accuracy on batch: 1.000000\n",
      "Epoch: 0012    cost: 0.117389109 \n",
      "I0716 15:32:15.630835 4654771648 feedforward_robust.py:763] Epoch: 0012    cost: 0.117389109 \n",
      "Accuracy on batch: 1.000000\n",
      "I0716 15:32:15.631896 4654771648 feedforward_robust.py:764] Accuracy on batch: 1.000000\n",
      "Epoch: 0013    cost: 0.115276873 \n",
      "I0716 15:32:16.542860 4654771648 feedforward_robust.py:763] Epoch: 0013    cost: 0.115276873 \n",
      "Accuracy on batch: 1.000000\n",
      "I0716 15:32:16.543962 4654771648 feedforward_robust.py:764] Accuracy on batch: 1.000000\n",
      "Epoch: 0014    cost: 0.107102121 \n",
      "I0716 15:32:17.501050 4654771648 feedforward_robust.py:763] Epoch: 0014    cost: 0.107102121 \n",
      "Accuracy on batch: 1.000000\n",
      "I0716 15:32:17.502064 4654771648 feedforward_robust.py:764] Accuracy on batch: 1.000000\n",
      "Epoch: 0015    cost: 0.101028066 \n",
      "I0716 15:32:18.488442 4654771648 feedforward_robust.py:763] Epoch: 0015    cost: 0.101028066 \n",
      "Accuracy on batch: 1.000000\n",
      "I0716 15:32:18.489697 4654771648 feedforward_robust.py:764] Accuracy on batch: 1.000000\n",
      "Epoch: 0016    cost: 0.095156263 \n",
      "I0716 15:32:19.629454 4654771648 feedforward_robust.py:763] Epoch: 0016    cost: 0.095156263 \n",
      "Accuracy on batch: 1.000000\n",
      "I0716 15:32:19.630558 4654771648 feedforward_robust.py:764] Accuracy on batch: 1.000000\n",
      "Epoch: 0017    cost: 0.090615200 \n",
      "I0716 15:32:20.933471 4654771648 feedforward_robust.py:763] Epoch: 0017    cost: 0.090615200 \n",
      "Accuracy on batch: 1.000000\n",
      "I0716 15:32:20.934735 4654771648 feedforward_robust.py:764] Accuracy on batch: 1.000000\n",
      "Epoch: 0018    cost: 0.088444341 \n",
      "I0716 15:32:22.597696 4654771648 feedforward_robust.py:763] Epoch: 0018    cost: 0.088444341 \n",
      "Accuracy on batch: 1.000000\n",
      "I0716 15:32:22.599256 4654771648 feedforward_robust.py:764] Accuracy on batch: 1.000000\n",
      "Epoch: 0019    cost: 0.087578990 \n",
      "I0716 15:32:24.374895 4654771648 feedforward_robust.py:763] Epoch: 0019    cost: 0.087578990 \n",
      "Accuracy on batch: 1.000000\n",
      "I0716 15:32:24.375991 4654771648 feedforward_robust.py:764] Accuracy on batch: 1.000000\n",
      "Epoch: 0020    cost: 0.085361575 \n",
      "I0716 15:32:25.482740 4654771648 feedforward_robust.py:763] Epoch: 0020    cost: 0.085361575 \n",
      "Accuracy on batch: 1.000000\n",
      "I0716 15:32:25.483767 4654771648 feedforward_robust.py:764] Accuracy on batch: 1.000000\n",
      "Optimization Finished!\n",
      "I0716 15:32:25.484870 4654771648 feedforward_robust.py:765] Optimization Finished!\n",
      "Final Train Loss 0.118301\n",
      "I0716 15:32:25.648520 4654771648 feedforward_robust.py:773] Final Train Loss 0.118301\n",
      "Final Train Accuracy 0.974833:\n",
      "I0716 15:32:25.649568 4654771648 feedforward_robust.py:774] Final Train Accuracy 0.974833:\n",
      "Model was trained on benign data\n",
      "I0716 15:32:25.651684 4654771648 feedforward_robust.py:796] Model was trained on benign data\n",
      "Model was evaluated on benign data\n",
      "I0716 15:32:25.733657 4654771648 feedforward_robust.py:675] Model was evaluated on benign data\n",
      "Model was evaluated on benign data\n",
      "I0716 15:32:25.822640 4654771648 feedforward_robust.py:675] Model was evaluated on benign data\n",
      "Model was evaluated on benign data\n",
      "I0716 15:32:25.866289 4654771648 feedforward_robust.py:675] Model was evaluated on benign data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----Regular test loss and accuracy ----\n",
      "(42.935635, 0.8207)\n",
      "----Real test loss and accuracy comparing to teacher ----\n",
      "(0.13625494, 0.9706)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model is being evaluated on FGSM data\n",
      "I0716 15:32:26.043533 4654771648 feedforward_robust.py:682] Model is being evaluated on FGSM data\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-d2cef4d2ef04>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslave_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-18-9eca7f10c2e6>\u001b[0m in \u001b[0;36mslave_training\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_real_reg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_real_reg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0mloss_fgsm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_fgsm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madv_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test_flat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpgd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"----FGSM test loss and accuracy ----\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_fgsm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_fgsm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "tup = slave_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(tup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "df['mse on z_train'] = []\n",
    "df['acc on z_train'] = []\n",
    "df['mse on z_test'] = []\n",
    "df['acc on z_test'] = []\n",
    "df['acc on y_test'] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_confidences = []\n",
    "test_confidences = []\n",
    "for i in range(3):\n",
    "    loss_real_train, acc_train, loss_real_reg, acc_real_reg, acc_reg, slave_train_confidences, slave_test_confidences = slave_training()\n",
    "    df.loc[i] = [loss_real_train, acc_train, loss_real_reg, acc_real_reg, acc_reg]\n",
    "    train_confidences.append(slave_train_confidences)\n",
    "    test_confidences.append(slave_test_confidences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_excel(\"ts_fashion_results.xlsx\")  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
